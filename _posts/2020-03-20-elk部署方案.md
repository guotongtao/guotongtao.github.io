---
layout: post
title: "elk部署"
categories: elasticsearch logstash kibana
tags: elk logstash kibana
---

* content
{:toc}

## 1.ELK部署架构

​	**一、概述**

ELK 已经成为目前最流行的集中式日志解决方案，它主要是由Beats、Logstash、Elasticsearch、Kibana等组件组成，来共同完成实时日志的收集，存储，展示等一站式的解决方案。本文将会介绍ELK常见的架构以及相关问题解决。




1. Filebeat：Filebeat是一款轻量级，占用服务资源非常少的数据收集引擎，它是ELK家族的新成员，可以代替Logstash作为在应用服务器端的日志收集引擎，支持将收集到的数据输出到Kafka，Redis等队列。

 

2. Logstash：数据收集引擎，相较于Filebeat比较重量级，但它集成了大量的插件，支持丰富的数据源收集，对收集的数据可以过滤，分析，格式化日志格式。

 

3. Elasticsearch：分布式数据搜索引擎，基于Apache Lucene实现，可集群，提供数据的集中式存储，分析，以及强大的数据搜索和聚合功能。

 

4. 4Kibana：数据的可视化平台，通过该web平台可以实时的查看 Elasticsearch 中的相关数据，并提供了丰富的图表统计功能。

**二、ELK常见部署架构**

**2.1 Logstash作为日志收集器**

这种架构是比较原始的部署架构，在各应用服务器端分别部署一个Logstash组件，作为日志收集器，然后将Logstash收集到的数据过滤、分析、格式化处理后发送至Elasticsearch存储，最后使用Kibana进行可视化展示，这种架构不足的是：Logstash比较耗服务器资源，所以会增加应用服务器端的负载压力。
![img](http://mmbiz.qpic.cn/mmbiz_png/icNyEYk3VqGk2JcFsJS8uM1z0uczq4ApHzXRQ0J6YhHib8ZH8vbvqLEQz7kjmdGjZcUpaibxs7g5icKVMFP8zjibgCg/0?wx_fmt=png)

 

**2.2 Filebeat作为日志收集器**

该架构与第一种架构唯一不同的是：应用端日志收集器换成了Filebeat，Filebeat轻量，占用服务器资源少，所以使用Filebeat作为应用服务器端的日志收集器，一般Filebeat会配合Logstash一起使用，这种部署方式也是目前最常用的架构。

 

![img](http://mmbiz.qpic.cn/mmbiz_png/icNyEYk3VqGk2JcFsJS8uM1z0uczq4ApHDfbTWAiaNq7UoIgCLRtO9UMXXlTog2diajJGV15tqLMFudG9J86msMMQ/0?wx_fmt=png)

 

**2.3 引入缓存队列的部署架构**

该架构在第二种架构的基础上引入了Kafka消息队列（还可以是其他消息队列），将Filebeat收集到的数据发送至Kafka，然后在通过Logstasth读取Kafka中的数据，这种架构主要是解决大数据量下的日志收集方案，使用缓存队列主要是解决数据安全与均衡Logstash与Elasticsearch负载压力。

 

 

![img](http://mmbiz.qpic.cn/mmbiz_png/icNyEYk3VqGk2JcFsJS8uM1z0uczq4ApHGn4mJQWMtkCEeXBAKdTuFcBkEAWJGIYnuegTlrnIQZXWBEtZQ1hXsQ/0?wx_fmt=png)

 

**2.4 以上三种架构的总结**

第一种部署架构由于资源占用问题，现已很少使用，目前使用最多的是第二种部署架构，至于第三种部署架构个人觉得没有必要引入消息队列，除非有其他需求，因为在数据量较大的情况下，Filebeat 使用压力敏感协议向 Logstash 或 Elasticsearch 发送数据。如果 Logstash 正在繁忙地处理数据，它会告知 Filebeat 减慢读取速度。拥塞解决后，Filebeat 将恢复初始速度并继续发送数据。

本文采用第二种方案


**三、问题及解决方案**

**问题：如何实现日志的多行合并功能？**

系统应用中的日志一般都是以特定格式进行打印的，属于同一条日志的数据可能分多行进行打印，那么在使用ELK收集日志的时候就需要将属于同一条日志的多行数据进行合并。

**解决方案：使用Filebeat或Logstash中的multiline多行合并插件来实现**

 

在使用multiline多行合并插件的时候需要注意，不同的ELK部署架构可能multiline的使用方式也不同，如果是本文的第一种部署架构，那么multiline需要在Logstash中配置使用，如果是第二种部署架构，那么multiline需要在Filebeat中配置使用，无需再在Logstash中配置multiline。

1、multiline在Filebeat中的配置方式：

 

> filebeat.prospectors:
>     \-
>        paths:
>           \- /home/project/elk/logs/test.log
>        input_type: log 
>        multiline:
>             pattern: '^\['
>             negate: true
>             match: after
> output:
>    logstash:
>       hosts: ["localhost:5044"]

 

> - pattern：正则表达式
> - negate：默认为false，表示匹配pattern的行合并到上一行；true表示不匹配pattern的行合并到上一行
> - match：after表示合并到上一行的末尾，before表示合并到上一行的行首

 

如：

 

> pattern: '\['
> negate: true
> match: after

该配置表示将不匹配pattern模式的行合并到上一行的末尾

2、multiline在Logstash中的配置方式

 

> input {
>   beats {
>     port => 5044
>   }
> }
>
> filter {
>   multiline {
>     pattern => "%{LOGLEVEL}\s*\]"
>     negate => true
>     what => "previous"
>   }
> }
>
> output {
>   elasticsearch {
>     hosts => "localhost:9200"
>   }
> }

 

> （1）Logstash中配置的what属性值为previous，相当于Filebeat中的after，Logstash中配置的what属性值为next，相当于Filebeat中的before。
> （2）pattern => "%{LOGLEVEL}\s*\]" 中的LOGLEVEL是Logstash预制的正则匹配模式，预制的还有好多常用的正则匹配模式，详细请看：https://github.com/logstash-plugins/logstash-patterns-core/tree/master/patterns

 

**问题：如何将Kibana中显示日志的时间字段替换为日志信息中的时间？**

默认情况下，我们在Kibana中查看的时间字段与日志信息中的时间不一致，因为默认的时间字段值是日志收集时的当前时间，所以需要将该字段的时间替换为日志信息中的时间。

**解决方案：使用grok分词插件与date时间格式化插件来实现**

在Logstash的配置文件的过滤器中配置grok分词插件与date时间格式化插件，如：

 

> input {
>   beats {
>     port => 5044
>   }
> }
>
> filter {
>   multiline {
>     pattern => "%{LOGLEVEL}\s*\]\[%{YEAR}%{MONTHNUM}%{MONTHDAY}\s+%{TIME}\]"
>     negate => true
>     what => "previous"
>   }
>
>   grok {
>     match => [ "message" , "(?<customer_time>%{YEAR}%{MONTHNUM}%{MONTHDAY}\s+%{TIME})" ]
>   }
>
>   date {
>         match => ["customer_time", "yyyyMMdd HH:mm:ss,SSS"] //格式化时间
>         target => "@timestamp" //替换默认的时间字段
>   }
> }
>
> output {
>   elasticsearch {
>     hosts => "localhost:9200"
>   }
> }

 

如要匹配的日志格式为：“[DEBUG][20170811 10:07:31,359][DefaultBeanDefinitionDocumentReader:106] Loading bean definitions”，解析出该日志的时间字段的方式有：

① 通过引入写好的表达式文件，如表达式文件为customer_patterns，内容为：
CUSTOMER_TIME %{YEAR}%{MONTHNUM}%{MONTHDAY}\s+%{TIME}

 

> **注：**内容格式为：[自定义表达式名称] [正则表达式]

 

然后logstash中就可以这样引用：

 

> filter {
>   grok {
>       patterns_dir => ["./customer-patterms/mypatterns"] //引用表达式文件路径
>       match => [ "message" , "%{CUSTOMER_TIME:customer_time}" ] //使用自定义的grok表达式
>   }
> }

 

② 以配置项的方式，规则为：(?<自定义表达式名称>正则匹配规则)，如：

 

> filter {
>   grok {
>     match => [ "message" , "(?<customer_time>%{YEAR}%{MONTHNUM}%{MONTHDAY}\s+%{TIME})" ]
>   }
> }

 

**问题：如何在Kibana中通过选择不同的系统日志模块来查看数据**

一般在Kibana中显示的日志数据混合了来自不同系统模块的数据，那么如何来选择或者过滤只查看指定的系统模块的日志数据？

**解决方案：新增标识不同系统模块的字段或根据不同系统模块建ES索引**

1、新增标识不同系统模块的字段，然后在Kibana中可以根据该字段来过滤查询不同模块的数据，这里以第二种部署架构讲解，在Filebeat中的配置内容为：

 

> filebeat.prospectors:
>     \-
>        paths:
>           \- /home/project/elk/logs/account.log
>        input_type: log 
>        multiline:
>             pattern: '^\['
>             negate: true
>             match: after
>        fields: //新增log_from字段
>          log_from: account
>
> ​    \-
> ​       paths:
> ​          \- /home/project/elk/logs/customer.log
> ​       input_type: log 
> ​       multiline:
> ​            pattern: '^\['
> ​            negate: true
> ​            match: after
> ​       fields:
> ​         log_from: customer
> output:
>    logstash:
> ​      hosts: ["localhost:5044"]

 

> 通过新增：log_from字段来标识不同的系统模块日志

 

2、根据不同的系统模块配置对应的ES索引，然后在Kibana中创建对应的索引模式匹配，即可在页面通过索引模式下拉框选择不同的系统模块数据。

 

这里以第二种部署架构讲解，分为两步：

 

① 在Filebeat中的配置内容为：

 

> filebeat.prospectors:
>     \-
>        paths:
>           \- /home/project/elk/logs/account.log
>        input_type: log 
>        multiline:
>             pattern: '^\['
>             negate: true
>             match: after
>        document_type: account
>
> ​    \-
> ​       paths:
> ​          \- /home/project/elk/logs/customer.log
> ​       input_type: log 
> ​       multiline:
> ​            pattern: '^\['
> ​            negate: true
> ​            match: after
> ​       document_type: customer
> output:
>    logstash:
> ​      hosts: ["localhost:5044"]

 

通过document_type来标识不同系统模块

② 修改Logstash中output的配置内容为：

 

> output {
>   elasticsearch {
>     hosts => "localhost:9200"
>     index => "%{type}"
>   }
> }

 

> 在output中增加index属性，%{type}表示按不同的document_type值建ES索引