---
layout: post
title: "ElasticSearch基本原理及使用"
categories: elk
tags: elasticsearch logstash filebeat kibana grok
---
* content
{:toc}
# 分布式实时日志分析解决方案

**一、概述**

 

ELK 已经成为目前最流行的集中式日志解决方案，它主要是由Beats、Logstash、Elasticsearch、Kibana等组件组成，来共同完成实时日志的收集，存储，展示等一站式的解决方案。本文将会介绍ELK常见的架构以及相关问题解决。

1. Filebeat：Filebeat是一款轻量级，占用服务资源非常少的数据收集引擎，它是ELK家族的新成员，可以代替Logstash作为在应用服务器端的日志收集引擎，支持将收集到的数据输出到Kafka，Redis等队列。

 

2. Logstash：数据收集引擎，相较于Filebeat比较重量级，但它集成了大量的插件，支持丰富的数据源收集，对收集的数据可以过滤，分析，格式化日志格式。

 

3. Elasticsearch：分布式数据搜索引擎，基于Apache Lucene实现，可集群，提供数据的集中式存储，分析，以及强大的数据搜索和聚合功能。

 

4. Kibana：数据的可视化平台，通过该web平台可以实时的查看 Elasticsearch 中的相关数据，并提供了丰富的图表统计功能。

**二、ELK常见部署架构**

 

**2.1 Logstash作为日志收集器**

这种架构是比较原始的部署架构，在各应用服务器端分别部署一个Logstash组件，作为日志收集器，然后将Logstash收集到的数据过滤、分析、格式化处理后发送至Elasticsearch存储，最后使用Kibana进行可视化展示，这种架构不足的是：Logstash比较耗服务器资源，所以会增加应用服务器端的负载压力。

 

 

![img](http://mmbiz.qpic.cn/mmbiz_png/icNyEYk3VqGk2JcFsJS8uM1z0uczq4ApHzXRQ0J6YhHib8ZH8vbvqLEQz7kjmdGjZcUpaibxs7g5icKVMFP8zjibgCg/0?wx_fmt=png)

 

**2.2 Filebeat作为日志收集器**

该架构与第一种架构唯一不同的是：应用端日志收集器换成了Filebeat，Filebeat轻量，占用服务器资源少，所以使用Filebeat作为应用服务器端的日志收集器，一般Filebeat会配合Logstash一起使用，这种部署方式也是目前最常用的架构。

 

![img](http://mmbiz.qpic.cn/mmbiz_png/icNyEYk3VqGk2JcFsJS8uM1z0uczq4ApHDfbTWAiaNq7UoIgCLRtO9UMXXlTog2diajJGV15tqLMFudG9J86msMMQ/0?wx_fmt=png)

 

**2.3 引入缓存队列的部署架构**

该架构在第二种架构的基础上引入了Kafka消息队列（还可以是其他消息队列），将Filebeat收集到的数据发送至Kafka，然后在通过Logstasth读取Kafka中的数据，这种架构主要是解决大数据量下的日志收集方案，使用缓存队列主要是解决数据安全与均衡Logstash与Elasticsearch负载压力。

 

 

![img](http://mmbiz.qpic.cn/mmbiz_png/icNyEYk3VqGk2JcFsJS8uM1z0uczq4ApHGn4mJQWMtkCEeXBAKdTuFcBkEAWJGIYnuegTlrnIQZXWBEtZQ1hXsQ/0?wx_fmt=png)

 

**2.4 以上三种架构的总结**

第一种部署架构由于资源占用问题，现已很少使用，目前使用最多的是第二种部署架构，至于第三种部署架构个人觉得没有必要引入消息队列，除非有其他需求，因为在数据量较大的情况下，Filebeat 使用压力敏感协议向 Logstash 或 Elasticsearch 发送数据。如果 Logstash 正在繁忙地处理数据，它会告知 Filebeat 减慢读取速度。拥塞解决后，Filebeat 将恢复初始速度并继续发送数据。

 

**三、问题及解决方案**

**问题：如何实现日志的多行合并功能？**

系统应用中的日志一般都是以特定格式进行打印的，属于同一条日志的数据可能分多行进行打印，那么在使用ELK收集日志的时候就需要将属于同一条日志的多行数据进行合并。

**解决方案：使用Filebeat或Logstash中的multiline多行合并插件来实现**

 

在使用multiline多行合并插件的时候需要注意，不同的ELK部署架构可能multiline的使用方式也不同，如果是本文的第一种部署架构，那么multiline需要在Logstash中配置使用，如果是第二种部署架构，那么multiline需要在Filebeat中配置使用，无需再在Logstash中配置multiline。

1、multiline在Filebeat中的配置方式：

 

> filebeat.prospectors:
>     \-
>        paths:
>           \- /home/project/elk/logs/test.log
>        input_type: log 
>        multiline:
>             pattern: '^\['
>             negate: true
>             match: after
> output:
>    logstash:
>       hosts: ["localhost:5044"]

 

> - pattern：正则表达式
> - negate：默认为false，表示匹配pattern的行合并到上一行；true表示不匹配pattern的行合并到上一行
> - match：after表示合并到上一行的末尾，before表示合并到上一行的行首

 

如：

 

> pattern: '\['
> negate: true
> match: after

 

该配置表示将不匹配pattern模式的行合并到上一行的末尾

2、multiline在Logstash中的配置方式

 

> input {
>   beats {
>     port => 5044
>   }
> }
>
> filter {
>   multiline {
>     pattern => "%{LOGLEVEL}\s*\]"
>     negate => true
>     what => "previous"
>   }
> }
>
> output {
>   elasticsearch {
>     hosts => "localhost:9200"
>   }
> }

 

> （1）Logstash中配置的what属性值为previous，相当于Filebeat中的after，Logstash中配置的what属性值为next，相当于Filebeat中的before。
> （2）pattern => "%{LOGLEVEL}\s*\]" 中的LOGLEVEL是Logstash预制的正则匹配模式，预制的还有好多常用的正则匹配模式，详细请看：https://github.com/logstash-plugins/logstash-patterns-core/tree/master/patterns

 

**问题：如何将Kibana中显示日志的时间字段替换为日志信息中的时间？**

默认情况下，我们在Kibana中查看的时间字段与日志信息中的时间不一致，因为默认的时间字段值是日志收集时的当前时间，所以需要将该字段的时间替换为日志信息中的时间。

**解决方案：使用grok分词插件与date时间格式化插件来实现**

在Logstash的配置文件的过滤器中配置grok分词插件与date时间格式化插件，如：

 

> input {
>   beats {
>     port => 5044
>   }
> }
>
> filter {
>   multiline {
>     pattern => "%{LOGLEVEL}\s*\]\[%{YEAR}%{MONTHNUM}%{MONTHDAY}\s+%{TIME}\]"
>     negate => true
>     what => "previous"
>   }
>
>   grok {
>     match => [ "message" , "(?<customer_time>%{YEAR}%{MONTHNUM}%{MONTHDAY}\s+%{TIME})" ]
>   }
>
>   date {
>         match => ["customer_time", "yyyyMMdd HH:mm:ss,SSS"] //格式化时间
>         target => "@timestamp" //替换默认的时间字段
>   }
> }
>
> output {
>   elasticsearch {
>     hosts => "localhost:9200"
>   }
> }

 

如要匹配的日志格式为：“[DEBUG][20170811 10:07:31,359][DefaultBeanDefinitionDocumentReader:106] Loading bean definitions”，解析出该日志的时间字段的方式有：

① 通过引入写好的表达式文件，如表达式文件为customer_patterns，内容为：
CUSTOMER_TIME %{YEAR}%{MONTHNUM}%{MONTHDAY}\s+%{TIME}

 

> **注：**内容格式为：[自定义表达式名称] [正则表达式]

 

然后logstash中就可以这样引用：

 

> filter {
>   grok {
>       patterns_dir => ["./customer-patterms/mypatterns"] //引用表达式文件路径
>       match => [ "message" , "%{CUSTOMER_TIME:customer_time}" ] //使用自定义的grok表达式
>   }
> }

 

② 以配置项的方式，规则为：(?<自定义表达式名称>正则匹配规则)，如：

 

> filter {
>   grok {
>     match => [ "message" , "(?<customer_time>%{YEAR}%{MONTHNUM}%{MONTHDAY}\s+%{TIME})" ]
>   }
> }

 

**问题：如何在Kibana中通过选择不同的系统日志模块来查看数据**

一般在Kibana中显示的日志数据混合了来自不同系统模块的数据，那么如何来选择或者过滤只查看指定的系统模块的日志数据？

**解决方案：新增标识不同系统模块的字段或根据不同系统模块建ES索引**

1、新增标识不同系统模块的字段，然后在Kibana中可以根据该字段来过滤查询不同模块的数据，这里以第二种部署架构讲解，在Filebeat中的配置内容为：

 

> filebeat.prospectors:
>     \-
>        paths:
>           \- /home/project/elk/logs/account.log
>        input_type: log 
>        multiline:
>             pattern: '^\['
>             negate: true
>             match: after
>        fields: //新增log_from字段
>          log_from: account
>
> ​    \-
> ​       paths:
> ​          \- /home/project/elk/logs/customer.log
> ​       input_type: log 
> ​       multiline:
> ​            pattern: '^\['
> ​            negate: true
> ​            match: after
> ​       fields:
> ​         log_from: customer
> output:
>    logstash:
> ​      hosts: ["localhost:5044"]

 

> 通过新增：log_from字段来标识不同的系统模块日志

 

2、根据不同的系统模块配置对应的ES索引，然后在Kibana中创建对应的索引模式匹配，即可在页面通过索引模式下拉框选择不同的系统模块数据。

 

这里以第二种部署架构讲解，分为两步：

 

① 在Filebeat中的配置内容为：

 

> filebeat.prospectors:
>     \-
>        paths:
>           \- /home/project/elk/logs/account.log
>        input_type: log 
>        multiline:
>             pattern: '^\['
>             negate: true
>             match: after
>        document_type: account
>
> ​    \-
> ​       paths:
> ​          \- /home/project/elk/logs/customer.log
> ​       input_type: log 
> ​       multiline:
> ​            pattern: '^\['
> ​            negate: true
> ​            match: after
> ​       document_type: customer
> output:
>    logstash:
> ​      hosts: ["localhost:5044"]

 

通过document_type来标识不同系统模块

② 修改Logstash中output的配置内容为：

 

> output {
>   elasticsearch {
>     hosts => "localhost:9200"
>     index => "%{type}"
>   }
> }

 

> 在output中增加index属性，%{type}表示按不同的document_type值建ES索引

 

**四、总结**

 

本文主要介绍了ELK实时日志分析的三种部署架构，以及不同架构所能解决的问题，这三种架构中第二种部署方式是时下最流行也是最常用的部署方式，最后介绍了ELK作在日志分析中的一些问题与解决方案，说在最后，ELK不仅仅可以用来作为分布式日志数据集中式查询和管理，还可以用来作为项目应用以及服务器资源监控等场景，更多内容请看官网。

 

> **出处：https://my.oschina.net/feinik/blog/1580625**

# filebeat

## filebeat中文配置文件详解

```################### Filebeat Configuration Example #########################

############################# Filebeat ######################################
filebeat:
  # List of prospectors to fetch data.
  prospectors:
    # Each - is a prospector. Below are the prospector specific configurations
    -
      # Paths that should be crawled and fetched. Glob based paths.
      # To fetch all ".log" files from a specific level of subdirectories
      # /var/log/*/*.log can be used.
      # For each file found under this path, a harvester is started.
      # Make sure not file is defined twice as this can lead to unexpected behaviour.
      # 指定要监控的日志，可以指定具体得文件或者目录
      paths:
        - /var/log/*.log  （这是默认的）（自行可以修改）(比如我放在/home/hadoop/app.log里）
        #- c:\programdata\elasticsearch\logs\*

      # Configure the file encoding for reading files with international characters
      # following the W3C recommendation for HTML5 (http://www.w3.org/TR/encoding).
      # Some sample encodings:
      #   plain, utf-8, utf-16be-bom, utf-16be, utf-16le, big5, gb18030, gbk,
      #    hz-gb-2312, euc-kr, euc-jp, iso-2022-jp, shift-jis, ...
      # 指定被监控的文件的编码类型，使用plain和utf-8都是可以处理中文日志的
      #encoding: plain

      # Type of the files. Based on this the way the file is read is decided.
      # The different types cannot be mixed in one prospector
      #
      # Possible options are:
      # * log: Reads every line of the log file (default)
      # * stdin: Reads the standard in
      # 指定文件的输入类型log(默认)或者stdin
      input_type: log

      # Exclude lines. A list of regular expressions to match. It drops the lines that are
      # matching any regular expression from the list. The include_lines is called before
      # 在输入中排除符合正则表达式列表的那些行。
      # exclude_lines. By default, no lines are dropped.
      # exclude_lines: ["^DBG"]

      # Include lines. A list of regular expressions to match. It exports the lines that are
      # matching any regular expression from the list. The include_lines is called before
      # exclude_lines. By default, all the lines are exported.
      # 包含输入中符合正则表达式列表的那些行（默认包含所有行），include_lines执行完毕之后会执行exclude_lines
      # include_lines: ["^ERR", "^WARN"]

      # Exclude files. A list of regular expressions to match. Filebeat drops the files that
      # are matching any regular expression from the list. By default, no files are dropped.
      # 忽略掉符合正则表达式列表的文件
      # exclude_files: [".gz$"]

      # Optional additional fields. These field can be freely picked
      # to add additional information to the crawled log files for filtering
      # 向输出的每一条日志添加额外的信息，比如“level:debug”，方便后续对日志进行分组统计。
      # 默认情况下，会在输出信息的fields子目录下以指定的新增fields建立子目录，例如fields.level
      # 这个得意思就是会在es中多添加一个字段，格式为 "filelds":{"level":"debug"}
      #fields:
      #  level: debug
      #  review: 1

      # Set to true to store the additional fields as top level fields instead
      # of under the "fields" sub-dictionary. In case of name conflicts with the
      # fields added by Filebeat itself, the custom fields overwrite the default
      # fields.
      # 如果该选项设置为true，则新增fields成为顶级目录，而不是将其放在fields目录下。
      # 自定义的field会覆盖filebeat默认的field
      # 如果设置为true，则在es中新增的字段格式为："level":"debug"
      #fields_under_root: false

      # Ignore files which were modified more then the defined timespan in the past.
      # In case all files on your system must be read you can set this value very large.
      # Time strings like 2h (2 hours), 5m (5 minutes) can be used.
      # 可以指定Filebeat忽略指定时间段以外修改的日志内容，比如2h（两个小时）或者5m(5分钟)。
      #ignore_older: 0

      # Close older closes the file handler for which were not modified
      # for longer then close_older
      # Time strings like 2h (2 hours), 5m (5 minutes) can be used.
      # 如果一个文件在某个时间段内没有发生过更新，则关闭监控的文件handle。默认1h
      #close_older: 1h

      # Type to be published in the 'type' field. For Elasticsearch output,
      # the type defines the document type these entries should be stored
      # in. Default: log
      # 设定Elasticsearch输出时的document的type字段 可以用来给日志进行分类。Default: log
      #document_type: log

      # Scan frequency in seconds.
      # How often these files should be checked for changes. In case it is set
      # to 0s, it is done as often as possible. Default: 10s
      # Filebeat以多快的频率去prospector指定的目录下面检测文件更新（比如是否有新增文件）
      # 如果设置为0s，则Filebeat会尽可能快地感知更新（占用的CPU会变高）。默认是10s
      #scan_frequency: 10s

      # Defines the buffer size every harvester uses when fetching the file
      # 每个harvester监控文件时，使用的buffer的大小
      #harvester_buffer_size: 16384

      # Maximum number of bytes a single log event can have
      # All bytes after max_bytes are discarded and not sent. The default is 10MB.
      # This is especially useful for multiline log messages which can get large.
      # 日志文件中增加一行算一个日志事件，max_bytes限制在一次日志事件中最多上传的字节数，多出的字节会被丢弃
      #max_bytes: 10485760

      # Mutiline can be used for log messages spanning multiple lines. This is common
      # for Java Stack Traces or C-Line Continuation
      # 适用于日志中每一条日志占据多行的情况，比如各种语言的报错信息调用栈
      #multiline:

        # The regexp Pattern that has to be matched. The example pattern matches all lines starting with [
        # 多行日志开始的那一行匹配的pattern
        #pattern: ^\[

        # Defines if the pattern set under pattern should be negated or not. Default is false.
        # 是否需要对pattern条件转置使用，不翻转设为true，反转设置为false。  【建议设置为true】
        #negate: false

        # Match can be set to "after" or "before". It is used to define if lines should be append to a pattern
        # that was (not) matched before or after or as long as a pattern is not matched based on negate.
        # Note: After is the equivalent to previous and before is the equivalent to to next in Logstash
        # 匹配pattern后，与前面（before）还是后面（after）的内容合并为一条日志
        #match: after

        # The maximum number of lines that are combined to one event.
        # In case there are more the max_lines the additional lines are discarded.
        # Default is 500
        # 合并的最多行数（包含匹配pattern的那一行）
        #max_lines: 500

        # After the defined timeout, an multiline event is sent even if no new pattern was found to start a new event
        # Default is 5s.
        # 到了timeout之后，即使没有匹配一个新的pattern（发生一个新的事件），也把已经匹配的日志事件发送出去
        #timeout: 5s

      # Setting tail_files to true means filebeat starts readding new files at the end
      # instead of the beginning. If this is used in combination with log rotation
      # this can mean that the first entries of a new file are skipped.
      # 如果设置为true，Filebeat从文件尾开始监控文件新增内容，把新增的每一行文件作为一个事件依次发送，
      # 而不是从文件开始处重新发送所有内容
      #tail_files: false

      # Backoff values define how agressively filebeat crawls new files for updates
      # The default values can be used in most cases. Backoff defines how long it is waited
      # to check a file again after EOF is reached. Default is 1s which means the file
      # is checked every second if new lines were added. This leads to a near real time crawling.
      # Every time a new line appears, backoff is reset to the initial value.
      # Filebeat检测到某个文件到了EOF（文件结尾）之后，每次等待多久再去检测文件是否有更新，默认为1s
      #backoff: 1s

      # Max backoff defines what the maximum backoff time is. After having backed off multiple times
      # from checking the files, the waiting time will never exceed max_backoff idenependent of the
      # backoff factor. Having it set to 10s means in the worst case a new line can be added to a log
      # file after having backed off multiple times, it takes a maximum of 10s to read the new line
      # Filebeat检测到某个文件到了EOF之后，等待检测文件更新的最大时间，默认是10秒
      #max_backoff: 10s

      # The backoff factor defines how fast the algorithm backs off. The bigger the backoff factor,
      # the faster the max_backoff value is reached. If this value is set to 1, no backoff will happen.
      # The backoff value will be multiplied each time with the backoff_factor until max_backoff is reached
      # 定义到达max_backoff的速度，默认因子是2，到达max_backoff后，变成每次等待max_backoff那么长的时间才backoff一次，
      # 直到文件有更新才会重置为backoff
      # 根据现在的默认配置是这样的，每隔1s检测一下文件变化，如果连续检测两次之后文件还没有变化，下一次检测间隔时间变为10s
      #backoff_factor: 2

      # This option closes a file, as soon as the file name changes.
      # This config option is recommended on windows only. Filebeat keeps the files it's reading open. This can cause
      # issues when the file is removed, as the file will not be fully removed until also Filebeat closes
      # the reading. Filebeat closes the file handler after ignore_older. During this time no new file with the
      # same name can be created. Turning this feature on the other hand can lead to loss of data
      # on rotate files. It can happen that after file rotation the beginning of the new
      # file is skipped, as the reading starts at the end. We recommend to leave this option on false
      # but lower the ignore_older value to release files faster.
      # 这个选项关闭一个文件,当文件名称的变化。#该配置选项建议只在windows
      #force_close_files: false

    # Additional prospector
    #-
      # Configuration to use stdin input
      #input_type: stdin

  # General filebeat configuration options
  #
  # Event count spool threshold - forces network flush if exceeded
  # spooler的大小，spooler中的事件数量超过这个阈值的时候会清空发送出去（不论是否到达超时时间）
  #spool_size: 2048

  # Enable async publisher pipeline in filebeat (Experimental!)
  # 是否采用异步发送模式（实验功能）
  #publish_async: false

  # Defines how often the spooler is flushed. After idle_timeout the spooler is
  # Flush even though spool_size is not reached.
  # spooler的超时时间，如果到了超时时间，spooler也会清空发送出去（不论是否到达容量的阈值）
  #idle_timeout: 5s

  # Name of the registry file. Per default it is put in the current working
  # directory. In case the working directory is changed after when running
  # filebeat again, indexing starts from the beginning again.
  # 记录filebeat处理日志文件的位置的文件，默认是在启动的根目录下
  #registry_file: .filebeat

  # Full Path to directory with additional prospector configuration files. Each file must end with .yml
  # These config files must have the full filebeat config part inside, but only
  # the prospector part is processed. All global options like spool_size are ignored.
  # The config_dir MUST point to a different directory then where the main filebeat config file is in.
  # 如果要在本配置文件中引入其他位置的配置文件，可以写在这里（需要写完整路径），但是只处理prospector的部分
  #config_dir:

###############################################################################
############################# Libbeat Config ##################################
# Base config file used by all other beats for using libbeat features

############################# Output ##########################################

# Configure what outputs to use when sending the data collected by the beat.
# Multiple outputs may be used.
output:

  ### Elasticsearch as output
  elasticsearch:　　　　　　　　　　　　（这是默认的，filebeat收集后放到es里）（自行可以修改，比如我有时候想filebeat收集后，然后到redis，再到es，就可以注销这行）
    # Array of hosts to connect to.
    # Scheme and port can be left out and will be set to the default (http and 9200)
    # In case you specify and additional path, the scheme is required: http://localhost:9200/path
    # IPv6 addresses should always be defined as: https://[2001:db8::1]:9200
    hosts: ["localhost:9200"]        （这是默认的，filebeat收集后放到es里）（自行可以修改，比如我有时候想filebeat收集后，然后到redis，再到es，就可以注销这行）
# Optional protocol and basic auth credentials. #protocol: "https" #username: "admin" #password: "s3cr3t" # Number of workers per Elasticsearch host. #worker: 1 # Optional index name. The default is "filebeat" and generates # [filebeat-]YYYY.MM.DD keys. #index: "filebeat" # A template is used to set the mapping in Elasticsearch # By default template loading is disabled and no template is loaded. # These settings can be adjusted to load your own template or overwrite existing ones #template: # Template name. By default the template name is filebeat. #name: "filebeat" # Path to template file #path: "filebeat.template.json" # Overwrite existing template #overwrite: false # Optional HTTP Path #path: "/elasticsearch" # Proxy server url #proxy_url: http://proxy:3128 # The number of times a particular Elasticsearch index operation is attempted. If # the indexing operation doesn't succeed after this many retries, the events are # dropped. The default is 3. #max_retries: 3 # The maximum number of events to bulk in a single Elasticsearch bulk API index request. # The default is 50. #bulk_max_size: 50 # Configure http request timeout before failing an request to Elasticsearch. #timeout: 90 # The number of seconds to wait for new events between two bulk API index requests. # If `bulk_max_size` is reached before this interval expires, addition bulk index # requests are made. #flush_interval: 1 # Boolean that sets if the topology is kept in Elasticsearch. The default is # false. This option makes sense only for Packetbeat. #save_topology: false # The time to live in seconds for the topology information that is stored in # Elasticsearch. The default is 15 seconds. #topology_expire: 15 # tls configuration. By default is off. #tls: # List of root certificates for HTTPS server verifications #certificate_authorities: ["/etc/pki/root/ca.pem"] # Certificate for TLS client authentication #certificate: "/etc/pki/client/cert.pem" # Client Certificate Key #certificate_key: "/etc/pki/client/cert.key" # Controls whether the client verifies server certificates and host name. # If insecure is set to true, all server host names and certificates will be # accepted. In this mode TLS based connections are susceptible to # man-in-the-middle attacks. Use only for testing. #insecure: true # Configure cipher suites to be used for TLS connections #cipher_suites: [] # Configure curve types for ECDHE based cipher suites #curve_types: [] # Configure minimum TLS version allowed for connection to logstash #min_version: 1.0 # Configure maximum TLS version allowed for connection to logstash #max_version: 1.2 ### Logstash as output #logstash: # The Logstash hosts #hosts: ["localhost:5044"] # Number of workers per Logstash host. #worker: 1 # The maximum number of events to bulk into a single batch window. The # default is 2048. #bulk_max_size: 2048 # Set gzip compression level. #compression_level: 3 # Optional load balance the events between the Logstash hosts #loadbalance: true # Optional index name. The default index name depends on the each beat. # For Packetbeat, the default is set to packetbeat, for Topbeat # top topbeat and for Filebeat to filebeat. #index: filebeat # Optional TLS. By default is off. #tls: # List of root certificates for HTTPS server verifications #certificate_authorities: ["/etc/pki/root/ca.pem"] # Certificate for TLS client authentication #certificate: "/etc/pki/client/cert.pem" # Client Certificate Key #certificate_key: "/etc/pki/client/cert.key" # Controls whether the client verifies server certificates and host name. # If insecure is set to true, all server host names and certificates will be # accepted. In this mode TLS based connections are susceptible to # man-in-the-middle attacks. Use only for testing. #insecure: true # Configure cipher suites to be used for TLS connections #cipher_suites: [] # Configure curve types for ECDHE based cipher suites #curve_types: [] ### File as output #file: # Path to the directory where to save the generated files. The option is mandatory. #path: "/tmp/filebeat" # Name of the generated files. The default is `filebeat` and it generates files: `filebeat`, `filebeat.1`, `filebeat.2`, etc. #filename: filebeat # Maximum size in kilobytes of each file. When this size is reached, the files are # rotated. The default value is 10 MB. #rotate_every_kb: 10000 # Maximum number of files under path. When this number of files is reached, the # oldest file is deleted and the rest are shifted from last to first. The default # is 7 files. #number_of_files: 7 ### Console output # console: # Pretty print json event #pretty: false ############################# Shipper ######################################### shipper: # The name of the shipper that publishes the network data. It can be used to group # all the transactions sent by a single shipper in the web interface. # If this options is not defined, the hostname is used. #name: # The tags of the shipper are included in their own field with each # transaction published. Tags make it easy to group servers by different # logical properties. #tags: ["service-X", "web-tier"] # Uncomment the following if you want to ignore transactions created # by the server on which the shipper is installed. This option is useful # to remove duplicates if shippers are installed on multiple servers. #ignore_outgoing: true # How often (in seconds) shippers are publishing their IPs to the topology map. # The default is 10 seconds. #refresh_topology_freq: 10 # Expiration time (in seconds) of the IPs published by a shipper to the topology map. # All the IPs will be deleted afterwards. Note, that the value must be higher than # refresh_topology_freq. The default is 15 seconds. #topology_expire: 15 # Internal queue size for single events in processing pipeline #queue_size: 1000 # Configure local GeoIP database support. # If no paths are not configured geoip is disabled. #geoip: #paths: # - "/usr/share/GeoIP/GeoLiteCity.dat" # - "/usr/local/var/GeoIP/GeoLiteCity.dat" ############################# Logging ######################################### # There are three options for the log ouput: syslog, file, stderr. # Under Windos systems, the log files are per default sent to the file output, # under all other system per default to syslog. # 建议在开发时期开启日志并把日志调整为debug或者info级别，在生产环境下调整为error级别 # 开启日志 必须设置to_files 属性为true logging: # Send all logging output to syslog. On Windows default is false, otherwise # default is true. # 配置beats日志。日志可以写入到syslog也可以是轮滚日志文件。默认是syslog # tail -f /var/log/messages #to_syslog: true # Write all logging output to files. Beats automatically rotate files if rotateeverybytes # limit is reached. # 日志发送到轮滚文件 #to_files: false # To enable logging to files, to_files option has to be set to true # to_files设置为true才可以开启轮滚日志记录 files: # The directory where the log files will written to. # 指定日志路径 #path: /var/log/mybeat # The name of the files where the logs are written to. # 指定日志名称 #name: mybeat # Configure log file size limit. If limit is reached, log file will be # automatically rotated # 默认文件达到10M就会滚动生成新文件 rotateeverybytes: 10485760 # = 10MB # Number of rotated log files to keep. Oldest files will be deleted first. # 保留日志文件周期。 默认 7天。值范围为2 到 1024 #keepfiles: 7 # Enable debug output for selected components. To enable all selectors use ["*"] # Other available selectors are beat, publish, service # Multiple selectors can be chained. #selectors: [ ] # Sets log level. The default log level is error. # Available log levels are: critical, error, warning, info, debug # 日志级别，默认是error #level: error
```

  

# logstash

## 配置文件自动重载工作原理

-  检测到配置文件变化
- 通过停止所有输入停止当前pipline
- 用新的配置创建一个新的管道
- 检查配置文件语法是否正确
- 检查所有的输入和输出是否可以初始化
- 检查成功使用新的pipeline替换当前的pipeline,
- 检查失败,使用旧的继续工作.
- 在重载过程中,jvm没有重启.

## logstash日志记录sincedb原理

由于测试玩耍，读取日志文件之后就不重新读取，需要每次把sincedb文件删除了，才会重新读取日志文件
低版本的Logstash2.4.0默认的sincedb文件目录在home目录下，通过ll -a就可以查看到
但是：Logstash6.5.4之后在home目录下没有找到
使用：find / -name .sincedb_*
查看到sincedb文件在/var/lib/logstash/plugins/inputs/file/.sincedb_d2343edad78a7252d2ea9cba15bbff6d
删除就能重新读取日志文件了，也可以指定指定sincedb文件的目录使用，重要该路径必须指定到文件不能指定到文件的目录
sincedb_path => "/home/logs/index.txt"
还有一种方式使用：利用linux黑洞可以达到每次重头读取日志文件
sincedb_path => "/dev/null"

如果Logstash重启，对于同一个文件，会继续从上次记录的位置开始读取。

## 使用Logstash的grok过滤日志文件

可以使用Logstash的grok模块对任意文本解析并结构化输出。Logstash默认带有120中匹配模式。

可以参见源代码

logstash/patterns/grok-patterns

logstash/lib/logstash/filters/grok.rb



grok的语法格式为 %{SYNTAX:SEMANTIC}

SYNTAX是文本要匹配的模式，例如3.14匹配 NUMBER 模式，127.0.0.1 匹配 IP 模式。



SEMANTIC 是匹配到的文本片段的标识。例如 “3.14” 可以是一个时间的持续时间，所以可以简单地叫做"duration" ，字符串"55.3.244.1"可以被标识为“client”

所以，grok过滤器表达式可以写成：



%{NUMBER:duration} %{IP:client}



默认情况下，所有的SEMANTIC是以字符串的方式保存，如果想要转换一个SEMANTIC的数据类型，例如转换一个字符串为×××，可以写成如下的方式：



%{NUMBER:num:int}



例如日志

55.3.244.1 GET /index.html 15824 0.043



可以写成如下的grok过滤表达式



%{IP:client} %{WORD:method} %{URIPATHPARAM:request} %{NUMBER:bytes} %{NUMBER:duration}

再举一个实际的案例

常规的Apache日志

```plain
127.0.0.1 - - [13/Apr/2015:17:22:03 +0800] "GET /router.php HTTP/1.1" 404 285 "-" "curl/7.19.7 (x86_64-redhat-linux-gnu) libcurl/7.19.7 NSS/3.15.3 zlib/1.2.3 libidn/1.18 libssh2/1.4.2"
127.0.0.1 - - [13/Apr/2015:17:22:03 +0800] "GET /router.php HTTP/1.1" 404 285 "-" "curl/7.19.7 (x86_64-redhat-linux-gnu) libcurl/7.19.7 NSS/3.15.3 zlib/1.2.3 libidn/1.18 libssh2/1.4.2"
```

使用Logstash收集

```plain
 input{
 
  file {
    type => "apache"
    path => "/var/log/httpd/access_log"
    exclude => ["*.gz"]
    sincedb_path => "/dev/null"
 
       }
 
      }

output {
   stdout {
      codec => rubydebug
          }
        }
```

显示：

```plain
{
       "message" => "127.0.0.1 - - [13/Apr/2015:17:22:03 +0800] \"GET /router.php HTTP/1.1\" 404 285 \"-\" \"curl/7.19.7 (x86_64-redhat-linux-gnu) libcurl/7.19.7 NSS/3.15.3 zlib/1.2.3 libidn/1.18 libssh2/1.4.2\"",
      "@version" => "1",
    "@timestamp" => "2015-04-13T09:22:03.844Z",
          "type" => "apache",
          "host" => "xxxxxx",
          "path" => "/var/log/httpd/access_log"
}
{
       "message" => "127.0.0.1 - - [13/Apr/2015:17:22:03 +0800] \"GET /router.php HTTP/1.1\" 404 285 \"-\" \"curl/7.19.7 (x86_64-redhat-linux-gnu) libcurl/7.19.7 NSS/3.15.3 zlib/1.2.3 libidn/1.18 libssh2/1.4.2\"",
      "@version" => "1",
    "@timestamp" => "2015-04-13T09:22:03.844Z",
          "type" => "apache",
          "host" => "xxxxxx",
          "path" => "/var/log/httpd/access_log"
}
```

修改配置如下：





```plain
input {

  file {
    type => "apache"
    path => "/var/log/httpd/access_log"
    exclude => ["*.gz"]
    sincedb_path => "/dev/null"
    
       }

      }
filter {
  if [type] == "apache" {
     grok {
          match => ["message",  "%{COMBINEDAPACHELOG}"]
          }
                         }
       }

output {
   stdout {
      codec => rubydebug
          }
       }
```



显示：

```plain
{
        "message" => "127.0.0.1 - - [14/Apr/2015:09:53:40 +0800] \"GET /router.php HTTP/1.1\" 404 285 \"-\" \"curl/7.19.7 (x86_64-redhat-linux-gnu) libcurl/7.19.7 NSS/3.15.3 zlib/1.2.3 libidn/1.18 libssh2/1.4.2\"",
       "@version" => "1",
     "@timestamp" => "2015-04-14T01:53:57.182Z",
           "type" => "apache",
           "host" => "xxxxxxxx",
           "path" => "/var/log/httpd/access_log",
       "clientip" => "127.0.0.1",
          "ident" => "-",
           "auth" => "-",
      "timestamp" => "14/Apr/2015:09:53:40 +0800",
           "verb" => "GET",
        "request" => "/router.php",
    "httpversion" => "1.1",
       "response" => "404",
          "bytes" => "285",
       "referrer" => "\"-\"",
          "agent" => "\"curl/7.19.7 (x86_64-redhat-linux-gnu) libcurl/7.19.7 NSS/3.15.3 zlib/1.2.3 libidn/1.18 libssh2/1.4.2\""
}
{
        "message" => "127.0.0.1 - - [14/Apr/2015:09:53:40 +0800] \"GET /router.php HTTP/1.1\" 404 285 \"-\" \"curl/7.19.7 (x86_64-redhat-linux-gnu) libcurl/7.19.7 NSS/3.15.3 zlib/1.2.3 libidn/1.18 libssh2/1.4.2\"",
       "@version" => "1",
     "@timestamp" => "2015-04-14T01:53:57.187Z",
           "type" => "apache",
           "host" => "xxxxxxx",
           "path" => "/var/log/httpd/access_log",
       "clientip" => "127.0.0.1",
          "ident" => "-",
           "auth" => "-",
      "timestamp" => "14/Apr/2015:09:53:40 +0800",
           "verb" => "GET",
        "request" => "/router.php",
    "httpversion" => "1.1",
       "response" => "404",
          "bytes" => "285",
       "referrer" => "\"-\"",
          "agent" => "\"curl/7.19.7 (x86_64-redhat-linux-gnu) libcurl/7.19.7 NSS/3.15.3 zlib/1.2.3 libidn/1.18 libssh2/1.4.2\""
}
```



这里的%{COMBINEDAPACHELOG} 是logstash自带的匹配模式

more /usr/share/logstash/vendor/bundle/jruby/2.5.0/gems/logstash-patterns-core-4.1.2/patterns/grok-patterns



```plain
COMMONAPACHELOG %{IPORHOST:clientip} %{USER:ident} %{USER:auth} \[%{HTTPDATE:timestamp}\] "(?:%{WORD:verb} %{NOTSPACE:req
uest}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})" %{NUMBER:response} (?:%{NUMBER:bytes}|-)
COMBINEDAPACHELOG %{COMMONAPACHELOG} %{QS:referrer} %{QS:agent}
```



grok可以支持任意正则表达式

所以支持的正则表达式的语法可以参见

http://www.geocities.jp/kosako3/oniguruma/doc/RE.txt



在有些情况下自带的匹配模式无法满足需求，可以自定义一些匹配模式

首先可以根据正则表达式匹配文本片段



(?<field_name>the pattern here)



例如，postfix日志有一个字段表示 queue id，可以使用以下表达式进行匹配：



(?<queue_id>[0-9A-F]{10,11}



可以手动创建一个匹配文件

\#   # contents of ./patterns/postfix:

   POSTFIX_QUEUEID [0-9A-F]{10,11}

```plain
    Jan  1 06:25:43 mailserver14 postfix/cleanup[21403]: BEF25A72965: message-id=<20130101142543.5828399CCAF@mailserver14.example.com>

     filter {
       grok {
         patterns_dir => "./patterns"
         match => [ "message", "%{SYSLOGBASE} %{POSTFIX_QUEUEID:queue_id}: %{GREEDYDATA:syslog_message}" ]
       }
     }

 The above will match and result in the following fields:

 * timestamp: Jan  1 06:25:43
 * logsource: mailserver14
 * program: postfix/cleanup
 * pid: 21403
 * queue_id: BEF25A72965
 * syslog_message: message-id=<20130101142543.5828399CCAF@mailserver14.example.com>

 The `timestamp`, `logsource`, `program`, and `pid` fields come from the
 SYSLOGBASE pattern which itself is defined by other patterns.
```



可以使用重写

```plain
The fields to overwrite.
 
 This allows you to overwrite a value in a field that already exists.
 
   For example, if you have a syslog line in the 'message' field, you can
   overwrite the 'message' field with part of the match like so:
  
       filter {
         grok {
           match => [
             "message",
             "%{SYSLOGBASE} %{DATA:message}"
           ]
           overwrite => [ "message" ]
         }
       }
  
    In this case, a line like "May 29 16:37:11 sadness logger: hello world"
    will be parsed and 'hello world' will overwrite the original message.
```

参考文档：

http://logstash.net/docs/1.4.2/filters/grok

https://github.com/logstash/logstash/tree/v1.4.2/patterns

# grok

## grok模式参考

参考：

https://help.aliyun.com/document_detail/135045.html?spm=5176.11065259.1996646101.searchclickresult.6c9338bfo4HTnK

GROK是一种采用组合多个预定义的正则表达式，用来匹配分割文本并映射到关键字的工具。通常用来对日志数据进行处理。本文档主要介绍GROK的模式说明以及常用语法。

GROK模式及说明如下表所示。

| 类型               | 模式（pattern）                                              | 说明                                                |
| :----------------- | :----------------------------------------------------------- | :-------------------------------------------------- |
| 常见模式           | CHINAID                                                      | 匹配中国居民身份证号。                              |
| USERNAME           | 匹配字母、数字和`._-`组合。                                  |                                                     |
| USER               | 匹配字母、数字和`._-`组合。                                  |                                                     |
| EMAILLOCALPART     | 匹配邮箱从开头到@字符前内容，如123456@alibaba.com，匹配内容为123456。 |                                                     |
| EMAILADDRESS       | 匹配邮箱。                                                   |                                                     |
| HTTPDUSER          | 匹配邮箱或者用户名。                                         |                                                     |
| INT                | 匹配INT数字。                                                |                                                     |
| BASE10NUM          | 匹配十进制数。                                               |                                                     |
| NUMBER             | 匹配数字。                                                   |                                                     |
| BASE16NUM          | 匹配十六进制数。                                             |                                                     |
| BASE16FLOAT        | 匹配十六进制浮点数。                                         |                                                     |
| POSINT             | 匹配正整数。                                                 |                                                     |
| NONNEGINT          | 匹配非负整数。                                               |                                                     |
| WORD               | 匹配字母或一句话。                                           |                                                     |
| NOTSPACE           | 匹配非空格内容。                                             |                                                     |
| SPACE              | 匹配空格。                                                   |                                                     |
| DATA               | 匹配换行符。                                                 |                                                     |
| GREEDYDATA         | 匹配0个或多个除换行符。                                      |                                                     |
| QUOTEDSTRING       | 匹配引用内容如：`I am "Iron Man"`，会匹配`Iron Man`内容。    |                                                     |
| UUID               | 匹配UUID。                                                   |                                                     |
| Networking         | MAC                                                          | 匹配MAC地址。                                       |
| CISCOMAC           | 匹配CISCOMAC地址。                                           |                                                     |
| WINDOWSMAC         | 匹配WINDOWSMAC地址。                                         |                                                     |
| COMMONMAC          | 匹配COMMONMAC地址。                                          |                                                     |
| IPV6               | 匹配IPV6。                                                   |                                                     |
| IPV4               | 匹配IPV4。                                                   |                                                     |
| IP                 | 匹配IPV6或IPV4。                                             |                                                     |
| HOSTNAME           | 匹配HOSTNAME。                                               |                                                     |
| IPORHOST           | 匹配IP或HOSTNAME。                                           |                                                     |
| HOSTPORT           | 匹配IPORHOST或者POSTINT。                                    |                                                     |
| Paths              | PATH                                                         | 匹配UNIXPATH或者WINPATH。                           |
| UNIXPATH           | 匹配UNIXPATH。                                               |                                                     |
| WINPATH            | 匹配WINPATH。                                                |                                                     |
| URIPROTO           | 匹配URI中的头部分，如`http://hostname.domain.tld/_astats?application=&inf.name=eth0`会匹配到`http`。 |                                                     |
| TTY                | 匹配TTY路径。                                                |                                                     |
| URIHOST            | 匹配IPORHOST和POSINT。还是以`http://hostname.domain.tld/_astats?application=&inf.name=eth0`为例，会匹配到`hostname.domain.tld`。 |                                                     |
| URI                | 匹配内容中的URI。                                            |                                                     |
| 日期               | MONTH                                                        | 匹配数字或者月份英文缩写或者全拼等格式月份。        |
| MONTHNUM           | 匹配数字格式月份。                                           |                                                     |
| MONTHDAY           | 匹配月份中的day。                                            |                                                     |
| DAY                | 匹配星期的英文全拼或者缩写。                                 |                                                     |
| YEAR               | 匹配年份。                                                   |                                                     |
| 时间               | HOUR                                                         | 匹配小时。                                          |
| MINUTE             | 匹配分钟。                                                   |                                                     |
| SECOND             | 匹配秒。                                                     |                                                     |
| TIME               | 匹配完整的时间。                                             |                                                     |
| DATE_US            | 匹配MonthDay-Year链接符，也可以是`/`组合形式的日期。         |                                                     |
| DATE_EU            | 匹配MonthDay-Year链接符，也可以是`/`或者`.`组合形式的日期。  |                                                     |
| ISO8601_TIMEZONE   | 匹配ISO8601格式的Hour和Minute。                              |                                                     |
| ISO8601_SECOND     | 匹配ISO8601格式的Second。                                    |                                                     |
| TIMESTAMP_ISO8601  | 匹配ISO8601格式的Time。                                      |                                                     |
| DATE               | 匹配US或EU格式的时间。                                       |                                                     |
| DATESTAMP          | 匹配完整日期和时间。                                         |                                                     |
| TZ                 | 匹配UTC。                                                    |                                                     |
| DATESTAMP_RFC822   | 匹配RFC822格式时间。                                         |                                                     |
| DATESTAMP_RFC2822  | 匹配RFC2822格式时间。                                        |                                                     |
| DATESTAMP_OTHER    | 匹配其他格式时间。                                           |                                                     |
| DATESTAMP_EVENTLOG | 匹配EVENTLOG格式时间。                                       |                                                     |
| HTTPDERROR_DATE    | 匹配HTTPDERROR格式时间。                                     |                                                     |
| SYSLOG             | SYSLOGTIMESTAMP                                              | 匹配Syslog格式时间。                                |
| PROG               | 匹配program内容。                                            |                                                     |
| SYSLOGPROG         | 匹配program和pid内容。                                       |                                                     |
| SYSLOGHOST         | 匹配IPORHOST。                                               |                                                     |
| SYSLOGFACILITY     | 匹配facility。                                               |                                                     |
| HTTPDATE           | 匹配日期时间。                                               |                                                     |
| LOGFORMATL         | LOGFORMAT                                                    | 匹配Syslog，默认TraditionalFormat格式的Syslog日志。 |
| COMMONAPACHELOG    | 匹配commonApache日志。                                       |                                                     |
| COMBINEDAPACHELOG  | 匹配组合Apache日志。                                         |                                                     |
| HTTPD20_ERRORLOG   | 匹配HTTPD20日志。                                            |                                                     |
| HTTPD24_ERRORLOG   | 匹配HTTPD24日志。                                            |                                                     |
| HTTPD_ERRORLOG     | 匹配HTTPD日志。                                              |                                                     |
| LOGLEVELS          | LOGLEVELS                                                    | 匹配Log的Level。例如warn，debug等。                 |


  

