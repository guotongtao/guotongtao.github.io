---  
layout: post  
title: "elk总结"  
categories: elk  
tags: elasticsearch logstash filebeat kibana grok  
---  

* content  
{:toc}  
# 分布式实时日志分析解决方案  

**一、概述**  

ELK 已经成为目前最流行的集中式日志解决方案，它主要是由Beats、Logstash、Elasticsearch、Kibana等组件组成，来共同完成实时日志的收集，存储，展示等一站式的解决方案。本文将会介绍ELK常见的架构以及相关问题解决。  






1. Filebeat：Filebeat是一款轻量级，占用服务资源非常少的数据收集引擎，它是ELK家族的新成员，可以代替Logstash作为在应用服务器端的日志收集引擎，支持将收集到的数据输出到Kafka，Redis等队列。  

 

2. Logstash：数据收集引擎，相较于Filebeat比较重量级，但它集成了大量的插件，支持丰富的数据源收集，对收集的数据可以过滤，分析，格式化日志格式。  

 

3. Elasticsearch：分布式数据搜索引擎，基于Apache Lucene实现，可集群，提供数据的集中式存储，分析，以及强大的数据搜索和聚合功能。  

 

4. Kibana：数据的可视化平台，通过该web平台可以实时的查看 Elasticsearch 中的相关数据，并提供了丰富的图表统计功能。  

**二、ELK常见部署架构**  

 

**2.1 Logstash作为日志收集器**  

这种架构是比较原始的部署架构，在各应用服务器端分别部署一个Logstash组件，作为日志收集器，然后将Logstash收集到的数据过滤、分析、格式化处理后发送至Elasticsearch存储，最后使用Kibana进行可视化展示，这种架构不足的是：Logstash比较耗服务器资源，所以会增加应用服务器端的负载压力。  

 

 

![img](http://mmbiz.qpic.cn/mmbiz_png/icNyEYk3VqGk2JcFsJS8uM1z0uczq4ApHzXRQ0J6YhHib8ZH8vbvqLEQz7kjmdGjZcUpaibxs7g5icKVMFP8zjibgCg/0?wx_fmt=png)  

 

**2.2 Filebeat作为日志收集器**  

该架构与第一种架构唯一不同的是：应用端日志收集器换成了Filebeat，Filebeat轻量，占用服务器资源少，所以使用Filebeat作为应用服务器端的日志收集器，一般Filebeat会配合Logstash一起使用，这种部署方式也是目前最常用的架构。  

 

![img](http://mmbiz.qpic.cn/mmbiz_png/icNyEYk3VqGk2JcFsJS8uM1z0uczq4ApHDfbTWAiaNq7UoIgCLRtO9UMXXlTog2diajJGV15tqLMFudG9J86msMMQ/0?wx_fmt=png)  

 

**2.3 引入缓存队列的部署架构**  

该架构在第二种架构的基础上引入了Kafka消息队列（还可以是其他消息队列），将Filebeat收集到的数据发送至Kafka，然后在通过Logstasth读取Kafka中的数据，这种架构主要是解决大数据量下的日志收集方案，使用缓存队列主要是解决数据安全与均衡Logstash与Elasticsearch负载压力。  


![img](http://mmbiz.qpic.cn/mmbiz_png/icNyEYk3VqGk2JcFsJS8uM1z0uczq4ApHGn4mJQWMtkCEeXBAKdTuFcBkEAWJGIYnuegTlrnIQZXWBEtZQ1hXsQ/0?wx_fmt=png)  

 

**2.4 以上三种架构的总结**  

第一种部署架构由于资源占用问题，现已很少使用，目前使用最多的是第二种部署架构，至于第三种部署架构个人觉得没有必要引入消息队列，除非有其他需求，因为在数据量较大的情况下，Filebeat 使用压力敏感协议向 Logstash 或 Elasticsearch 发送数据。如果 Logstash 正在繁忙地处理数据，它会告知 Filebeat 减慢读取速度。拥塞解决后，Filebeat 将恢复初始速度并继续发送数据。  

 

**三、问题及解决方案**  

**问题：如何实现日志的多行合并功能？**  

系统应用中的日志一般都是以特定格式进行打印的，属于同一条日志的数据可能分多行进行打印，那么在使用ELK收集日志的时候就需要将属于同一条日志的多行数据进行合并。  

**解决方案：使用Filebeat或Logstash中的multiline多行合并插件来实现**  

 

在使用multiline多行合并插件的时候需要注意，不同的ELK部署架构可能multiline的使用方式也不同，如果是本文的第一种部署架构，那么multiline需要在Logstash中配置使用，如果是第二种部署架构，那么multiline需要在Filebeat中配置使用，无需再在Logstash中配置multiline。  

1、multiline在Filebeat中的配置方式：  

 

> filebeat.prospectors:  
>     \-  
>        paths:  
>           \- /home/project/elk/logs/test.log  
>        input_type: log  
>        multiline:  
>             pattern: '^\['  
>             negate: true  
>             match: after  
> output:  
>    logstash:  
>       hosts: ["localhost:5044"]  

 

> - pattern：正则表达式  
> - negate：默认为false，表示匹配pattern的行合并到上一行；true表示不匹配pattern的行合并到上一行  
> - match：after表示合并到上一行的末尾，before表示合并到上一行的行首  

 

如：  

 

> pattern: '\['  
> negate: true  
> match: after  

 

该配置表示将不匹配pattern模式的行合并到上一行的末尾  

2、multiline在Logstash中的配置方式  

 

> input {  
>   beats {  
>     port => 5044  
>   }  
> }  
>  
> filter {  
>   multiline {  
>     pattern => "%{LOGLEVEL}\s*\]"  
>     negate => true  
>     what => "previous"  
>   }  
> }  
>  
> output {  
>   elasticsearch {  
>     hosts => "localhost:9200"  
>   }  
> }  

 

> （1）Logstash中配置的what属性值为previous，相当于Filebeat中的after，Logstash中配置的what属性值为next，相当于Filebeat中的before。  
> （2）pattern => "%{LOGLEVEL}\s*\]" 中的LOGLEVEL是Logstash预制的正则匹配模式，预制的还有好多常用的正则匹配模式，详细请看：https://github.com/logstash-plugins/logstash-patterns-core/tree/master/patterns  

 

**问题：如何将Kibana中显示日志的时间字段替换为日志信息中的时间？**  

默认情况下，我们在Kibana中查看的时间字段与日志信息中的时间不一致，因为默认的时间字段值是日志收集时的当前时间，所以需要将该字段的时间替换为日志信息中的时间。  

**解决方案：使用grok分词插件与date时间格式化插件来实现**  

在Logstash的配置文件的过滤器中配置grok分词插件与date时间格式化插件，如：  

 

> input {  
>   beats {  
>     port => 5044  
>   }  
> }  
>  
> filter {  
>   multiline {  
>     pattern => "%{LOGLEVEL}\s*\]\[%{YEAR}%{MONTHNUM}%{MONTHDAY}\s+%{TIME}\]"  
>     negate => true  
>     what => "previous"  
>   }  
>  
>   grok {  
>     match => [ "message" , "(?<customer_time>%{YEAR}%{MONTHNUM}%{MONTHDAY}\s+%{TIME})" ]  
>   }  
>  
>   date {  
>         match => ["customer_time", "yyyyMMdd HH:mm:ss,SSS"] //格式化时间  
>         target => "@timestamp" //替换默认的时间字段  
>   }  
> }  
>  
> output {  
>   elasticsearch {  
>     hosts => "localhost:9200"  
>   }  
> }  

 

如要匹配的日志格式为：“[DEBUG][20170811 10:07:31,359][DefaultBeanDefinitionDocumentReader:106] Loading bean definitions”，解析出该日志的时间字段的方式有：  

① 通过引入写好的表达式文件，如表达式文件为customer_patterns，内容为：  
CUSTOMER_TIME %{YEAR}%{MONTHNUM}%{MONTHDAY}\s+%{TIME}  

 

> **注：**内容格式为：[自定义表达式名称] [正则表达式]  

 

然后logstash中就可以这样引用：  

 

> filter {  
>   grok {  
>       patterns_dir => ["./customer-patterms/mypatterns"] //引用表达式文件路径  
>       match => [ "message" , "%{CUSTOMER_TIME:customer_time}" ] //使用自定义的grok表达式  
>   }  
> }  

 

② 以配置项的方式，规则为：(?<自定义表达式名称>正则匹配规则)，如：  

 

> filter {  
>   grok {  
>     match => [ "message" , "(?<customer_time>%{YEAR}%{MONTHNUM}%{MONTHDAY}\s+%{TIME})" ]  
>   }  
> }  

 

**问题：如何在Kibana中通过选择不同的系统日志模块来查看数据**  

一般在Kibana中显示的日志数据混合了来自不同系统模块的数据，那么如何来选择或者过滤只查看指定的系统模块的日志数据？  

**解决方案：新增标识不同系统模块的字段或根据不同系统模块建ES索引**  

1、新增标识不同系统模块的字段，然后在Kibana中可以根据该字段来过滤查询不同模块的数据，这里以第二种部署架构讲解，在Filebeat中的配置内容为：  

 

> filebeat.prospectors:  
>     \-  
>        paths:  
>           \- /home/project/elk/logs/account.log  
>        input_type: log  
>        multiline:  
>             pattern: '^\['  
>             negate: true  
>             match: after  
>        fields: //新增log_from字段  
>          log_from: account  
>  
> ​    \-  
> ​       paths:  
> ​          \- /home/project/elk/logs/customer.log  
> ​       input_type: log  
> ​       multiline:  
> ​            pattern: '^\['  
> ​            negate: true  
> ​            match: after  
> ​       fields:  
> ​         log_from: customer  
> output:  
>    logstash:  
> ​      hosts: ["localhost:5044"]  

 

> 通过新增：log_from字段来标识不同的系统模块日志  

 

2、根据不同的系统模块配置对应的ES索引，然后在Kibana中创建对应的索引模式匹配，即可在页面通过索引模式下拉框选择不同的系统模块数据。  

 

这里以第二种部署架构讲解，分为两步：  

 

① 在Filebeat中的配置内容为：  

 

> filebeat.prospectors:  
>     \-  
>        paths:  
>           \- /home/project/elk/logs/account.log  
>        input_type: log  
>        multiline:  
>             pattern: '^\['  
>             negate: true  
>             match: after  
>        document_type: account  
>  
> ​    \-  
> ​       paths:  
> ​          \- /home/project/elk/logs/customer.log  
> ​       input_type: log  
> ​       multiline:  
> ​            pattern: '^\['  
> ​            negate: true  
> ​            match: after  
> ​       document_type: customer  
> output:  
>    logstash:  
> ​      hosts: ["localhost:5044"]  

 

通过document_type来标识不同系统模块  

② 修改Logstash中output的配置内容为：  

 

> output {  
>   elasticsearch {  
>     hosts => "localhost:9200"  
>     index => "%{type}"  
>   }  
> }  

 

> 在output中增加index属性，%{type}表示按不同的document_type值建ES索引  

 

**四、总结**  

 

本文主要介绍了ELK实时日志分析的三种部署架构，以及不同架构所能解决的问题，这三种架构中第二种部署方式是时下最流行也是最常用的部署方式，最后介绍了ELK作在日志分析中的一些问题与解决方案，说在最后，ELK不仅仅可以用来作为分布式日志数据集中式查询和管理，还可以用来作为项目应用以及服务器资源监控等场景，更多内容请看官网。  

 

> **出处：https://my.oschina.net/feinik/blog/1580625**  

# filebeat  

## filebeat中文配置文件详解  

```################### Filebeat Configuration Example #########################  

############################# Filebeat ######################################  
filebeat:  
  # List of prospectors to fetch data.  
  prospectors:  
    # Each - is a prospector. Below are the prospector specific configurations  
    -  
      # Paths that should be crawled and fetched. Glob based paths.  
      # To fetch all ".log" files from a specific level of subdirectories  
      # /var/log/*/*.log can be used.  
      # For each file found under this path, a harvester is started.  
      # Make sure not file is defined twice as this can lead to unexpected behaviour.  
      # 指定要监控的日志，可以指定具体得文件或者目录  
      paths:  
        - /var/log/*.log  （这是默认的）（自行可以修改）(比如我放在/home/hadoop/app.log里）  
        #- c:\programdata\elasticsearch\logs\*  

      # Configure the file encoding for reading files with international characters  
      # following the W3C recommendation for HTML5 (http://www.w3.org/TR/encoding).  
      # Some sample encodings:  
      #   plain, utf-8, utf-16be-bom, utf-16be, utf-16le, big5, gb18030, gbk,  
      #    hz-gb-2312, euc-kr, euc-jp, iso-2022-jp, shift-jis, ...  
      # 指定被监控的文件的编码类型，使用plain和utf-8都是可以处理中文日志的  
      #encoding: plain  

      # Type of the files. Based on this the way the file is read is decided.  
      # The different types cannot be mixed in one prospector  
      #  
      # Possible options are:  
      # * log: Reads every line of the log file (default)  
      # * stdin: Reads the standard in  
      # 指定文件的输入类型log(默认)或者stdin  
      input_type: log  

      # Exclude lines. A list of regular expressions to match. It drops the lines that are  
      # matching any regular expression from the list. The include_lines is called before  
      # 在输入中排除符合正则表达式列表的那些行。  
      # exclude_lines. By default, no lines are dropped.  
      # exclude_lines: ["^DBG"]  

      # Include lines. A list of regular expressions to match. It exports the lines that are  
      # matching any regular expression from the list. The include_lines is called before  
      # exclude_lines. By default, all the lines are exported.  
      # 包含输入中符合正则表达式列表的那些行（默认包含所有行），include_lines执行完毕之后会执行exclude_lines  
      # include_lines: ["^ERR", "^WARN"]  

      # Exclude files. A list of regular expressions to match. Filebeat drops the files that  
      # are matching any regular expression from the list. By default, no files are dropped.  
      # 忽略掉符合正则表达式列表的文件  
      # exclude_files: [".gz$"]  

      # Optional additional fields. These field can be freely picked  
      # to add additional information to the crawled log files for filtering  
      # 向输出的每一条日志添加额外的信息，比如“level:debug”，方便后续对日志进行分组统计。  
      # 默认情况下，会在输出信息的fields子目录下以指定的新增fields建立子目录，例如fields.level  
      # 这个得意思就是会在es中多添加一个字段，格式为 "filelds":{"level":"debug"}  
      #fields:  
      #  level: debug  
      #  review: 1  

      # Set to true to store the additional fields as top level fields instead  
      # of under the "fields" sub-dictionary. In case of name conflicts with the  
      # fields added by Filebeat itself, the custom fields overwrite the default  
      # fields.  
      # 如果该选项设置为true，则新增fields成为顶级目录，而不是将其放在fields目录下。  
      # 自定义的field会覆盖filebeat默认的field  
      # 如果设置为true，则在es中新增的字段格式为："level":"debug"  
      #fields_under_root: false  

      # Ignore files which were modified more then the defined timespan in the past.  
      # In case all files on your system must be read you can set this value very large.  
      # Time strings like 2h (2 hours), 5m (5 minutes) can be used.  
      # 可以指定Filebeat忽略指定时间段以外修改的日志内容，比如2h（两个小时）或者5m(5分钟)。  
      #ignore_older: 0  

      # Close older closes the file handler for which were not modified  
      # for longer then close_older  
      # Time strings like 2h (2 hours), 5m (5 minutes) can be used.  
      # 如果一个文件在某个时间段内没有发生过更新，则关闭监控的文件handle。默认1h  
      #close_older: 1h  

      # Type to be published in the 'type' field. For Elasticsearch output,  
      # the type defines the document type these entries should be stored  
      # in. Default: log  
      # 设定Elasticsearch输出时的document的type字段 可以用来给日志进行分类。Default: log  
      #document_type: log  

      # Scan frequency in seconds.  
      # How often these files should be checked for changes. In case it is set  
      # to 0s, it is done as often as possible. Default: 10s  
      # Filebeat以多快的频率去prospector指定的目录下面检测文件更新（比如是否有新增文件）  
      # 如果设置为0s，则Filebeat会尽可能快地感知更新（占用的CPU会变高）。默认是10s  
      #scan_frequency: 10s  

      # Defines the buffer size every harvester uses when fetching the file  
      # 每个harvester监控文件时，使用的buffer的大小  
      #harvester_buffer_size: 16384  

      # Maximum number of bytes a single log event can have  
      # All bytes after max_bytes are discarded and not sent. The default is 10MB.  
      # This is especially useful for multiline log messages which can get large.  
      # 日志文件中增加一行算一个日志事件，max_bytes限制在一次日志事件中最多上传的字节数，多出的字节会被丢弃  
      #max_bytes: 10485760  

      # Mutiline can be used for log messages spanning multiple lines. This is common  
      # for Java Stack Traces or C-Line Continuation  
      # 适用于日志中每一条日志占据多行的情况，比如各种语言的报错信息调用栈  
      #multiline:  

        # The regexp Pattern that has to be matched. The example pattern matches all lines starting with [  
        # 多行日志开始的那一行匹配的pattern  
        #pattern: ^\[  

        # Defines if the pattern set under pattern should be negated or not. Default is false.  
        # 是否需要对pattern条件转置使用，不翻转设为true，反转设置为false。  【建议设置为true】  
        #negate: false  

        # Match can be set to "after" or "before". It is used to define if lines should be append to a pattern  
        # that was (not) matched before or after or as long as a pattern is not matched based on negate.  
        # Note: After is the equivalent to previous and before is the equivalent to to next in Logstash  
        # 匹配pattern后，与前面（before）还是后面（after）的内容合并为一条日志  
        #match: after  

        # The maximum number of lines that are combined to one event.  
        # In case there are more the max_lines the additional lines are discarded.  
        # Default is 500  
        # 合并的最多行数（包含匹配pattern的那一行）  
        #max_lines: 500  

        # After the defined timeout, an multiline event is sent even if no new pattern was found to start a new event  
        # Default is 5s.  
        # 到了timeout之后，即使没有匹配一个新的pattern（发生一个新的事件），也把已经匹配的日志事件发送出去  
        #timeout: 5s  

      # Setting tail_files to true means filebeat starts readding new files at the end  
      # instead of the beginning. If this is used in combination with log rotation  
      # this can mean that the first entries of a new file are skipped.  
      # 如果设置为true，Filebeat从文件尾开始监控文件新增内容，把新增的每一行文件作为一个事件依次发送，  
      # 而不是从文件开始处重新发送所有内容  
      #tail_files: false  

      # Backoff values define how agressively filebeat crawls new files for updates  
      # The default values can be used in most cases. Backoff defines how long it is waited  
      # to check a file again after EOF is reached. Default is 1s which means the file  
      # is checked every second if new lines were added. This leads to a near real time crawling.  
      # Every time a new line appears, backoff is reset to the initial value.  
      # Filebeat检测到某个文件到了EOF（文件结尾）之后，每次等待多久再去检测文件是否有更新，默认为1s  
      #backoff: 1s  

      # Max backoff defines what the maximum backoff time is. After having backed off multiple times  
      # from checking the files, the waiting time will never exceed max_backoff idenependent of the  
      # backoff factor. Having it set to 10s means in the worst case a new line can be added to a log  
      # file after having backed off multiple times, it takes a maximum of 10s to read the new line  
      # Filebeat检测到某个文件到了EOF之后，等待检测文件更新的最大时间，默认是10秒  
      #max_backoff: 10s  

      # The backoff factor defines how fast the algorithm backs off. The bigger the backoff factor,  
      # the faster the max_backoff value is reached. If this value is set to 1, no backoff will happen.  
      # The backoff value will be multiplied each time with the backoff_factor until max_backoff is reached  
      # 定义到达max_backoff的速度，默认因子是2，到达max_backoff后，变成每次等待max_backoff那么长的时间才backoff一次，  
      # 直到文件有更新才会重置为backoff  
      # 根据现在的默认配置是这样的，每隔1s检测一下文件变化，如果连续检测两次之后文件还没有变化，下一次检测间隔时间变为10s  
      #backoff_factor: 2  

      # This option closes a file, as soon as the file name changes.  
      # This config option is recommended on windows only. Filebeat keeps the files it's reading open. This can cause  
      # issues when the file is removed, as the file will not be fully removed until also Filebeat closes  
      # the reading. Filebeat closes the file handler after ignore_older. During this time no new file with the  
      # same name can be created. Turning this feature on the other hand can lead to loss of data  
      # on rotate files. It can happen that after file rotation the beginning of the new  
      # file is skipped, as the reading starts at the end. We recommend to leave this option on false  
      # but lower the ignore_older value to release files faster.  
      # 这个选项关闭一个文件,当文件名称的变化。#该配置选项建议只在windows  
      #force_close_files: false  

    # Additional prospector  
    #-  
      # Configuration to use stdin input  
      #input_type: stdin  

  # General filebeat configuration options  
  #  
  # Event count spool threshold - forces network flush if exceeded  
  # spooler的大小，spooler中的事件数量超过这个阈值的时候会清空发送出去（不论是否到达超时时间）  
  #spool_size: 2048  

  # Enable async publisher pipeline in filebeat (Experimental!)  
  # 是否采用异步发送模式（实验功能）  
  #publish_async: false  

  # Defines how often the spooler is flushed. After idle_timeout the spooler is  
  # Flush even though spool_size is not reached.  
  # spooler的超时时间，如果到了超时时间，spooler也会清空发送出去（不论是否到达容量的阈值）  
  #idle_timeout: 5s  

  # Name of the registry file. Per default it is put in the current working  
  # directory. In case the working directory is changed after when running  
  # filebeat again, indexing starts from the beginning again.  
  # 记录filebeat处理日志文件的位置的文件，默认是在启动的根目录下  
  #registry_file: .filebeat  

  # Full Path to directory with additional prospector configuration files. Each file must end with .yml  
  # These config files must have the full filebeat config part inside, but only  
  # the prospector part is processed. All global options like spool_size are ignored.  
  # The config_dir MUST point to a different directory then where the main filebeat config file is in.  
  # 如果要在本配置文件中引入其他位置的配置文件，可以写在这里（需要写完整路径），但是只处理prospector的部分  
  #config_dir:  

###############################################################################  
############################# Libbeat Config ##################################  
# Base config file used by all other beats for using libbeat features  

############################# Output ##########################################  

# Configure what outputs to use when sending the data collected by the beat.  
# Multiple outputs may be used.  
output:  

  ### Elasticsearch as output  
  elasticsearch:　　　　　　　　　　　　（这是默认的，filebeat收集后放到es里）（自行可以修改，比如我有时候想filebeat收集后，然后到redis，再到es，就可以注销这行）  
    # Array of hosts to connect to.  
    # Scheme and port can be left out and will be set to the default (http and 9200)  
    # In case you specify and additional path, the scheme is required: http://localhost:9200/path  
    # IPv6 addresses should always be defined as: https://[2001:db8::1]:9200  
    hosts: ["localhost:9200"]        （这是默认的，filebeat收集后放到es里）（自行可以修改，比如我有时候想filebeat收集后，然后到redis，再到es，就可以注销这行）  
# Optional protocol and basic auth credentials. #protocol: "https" #username: "admin" #password: "s3cr3t" # Number of workers per Elasticsearch host. #worker: 1 # Optional index name. The default is "filebeat" and generates # [filebeat-]YYYY.MM.DD keys. #index: "filebeat" # A template is used to set the mapping in Elasticsearch # By default template loading is disabled and no template is loaded. # These settings can be adjusted to load your own template or overwrite existing ones #template: # Template name. By default the template name is filebeat. #name: "filebeat" # Path to template file #path: "filebeat.template.json" # Overwrite existing template #overwrite: false # Optional HTTP Path #path: "/elasticsearch" # Proxy server url #proxy_url: http://proxy:3128 # The number of times a particular Elasticsearch index operation is attempted. If # the indexing operation doesn't succeed after this many retries, the events are # dropped. The default is 3. #max_retries: 3 # The maximum number of events to bulk in a single Elasticsearch bulk API index request. # The default is 50. #bulk_max_size: 50 # Configure http request timeout before failing an request to Elasticsearch. #timeout: 90 # The number of seconds to wait for new events between two bulk API index requests. # If `bulk_max_size` is reached before this interval expires, addition bulk index # requests are made. #flush_interval: 1 # Boolean that sets if the topology is kept in Elasticsearch. The default is # false. This option makes sense only for Packetbeat. #save_topology: false # The time to live in seconds for the topology information that is stored in # Elasticsearch. The default is 15 seconds. #topology_expire: 15 # tls configuration. By default is off. #tls: # List of root certificates for HTTPS server verifications #certificate_authorities: ["/etc/pki/root/ca.pem"] # Certificate for TLS client authentication #certificate: "/etc/pki/client/cert.pem" # Client Certificate Key #certificate_key: "/etc/pki/client/cert.key" # Controls whether the client verifies server certificates and host name. # If insecure is set to true, all server host names and certificates will be # accepted. In this mode TLS based connections are susceptible to # man-in-the-middle attacks. Use only for testing. #insecure: true # Configure cipher suites to be used for TLS connections #cipher_suites: [] # Configure curve types for ECDHE based cipher suites #curve_types: [] # Configure minimum TLS version allowed for connection to logstash #min_version: 1.0 # Configure maximum TLS version allowed for connection to logstash #max_version: 1.2 ### Logstash as output #logstash: # The Logstash hosts #hosts: ["localhost:5044"] # Number of workers per Logstash host. #worker: 1 # The maximum number of events to bulk into a single batch window. The # default is 2048. #bulk_max_size: 2048 # Set gzip compression level. #compression_level: 3 # Optional load balance the events between the Logstash hosts #loadbalance: true # Optional index name. The default index name depends on the each beat. # For Packetbeat, the default is set to packetbeat, for Topbeat # top topbeat and for Filebeat to filebeat. #index: filebeat # Optional TLS. By default is off. #tls: # List of root certificates for HTTPS server verifications #certificate_authorities: ["/etc/pki/root/ca.pem"] # Certificate for TLS client authentication #certificate: "/etc/pki/client/cert.pem" # Client Certificate Key #certificate_key: "/etc/pki/client/cert.key" # Controls whether the client verifies server certificates and host name. # If insecure is set to true, all server host names and certificates will be # accepted. In this mode TLS based connections are susceptible to # man-in-the-middle attacks. Use only for testing. #insecure: true # Configure cipher suites to be used for TLS connections #cipher_suites: [] # Configure curve types for ECDHE based cipher suites #curve_types: [] ### File as output #file: # Path to the directory where to save the generated files. The option is mandatory. #path: "/tmp/filebeat" # Name of the generated files. The default is `filebeat` and it generates files: `filebeat`, `filebeat.1`, `filebeat.2`, etc. #filename: filebeat # Maximum size in kilobytes of each file. When this size is reached, the files are # rotated. The default value is 10 MB. #rotate_every_kb: 10000 # Maximum number of files under path. When this number of files is reached, the # oldest file is deleted and the rest are shifted from last to first. The default # is 7 files. #number_of_files: 7 ### Console output # console: # Pretty print json event #pretty: false ############################# Shipper ######################################### shipper: # The name of the shipper that publishes the network data. It can be used to group # all the transactions sent by a single shipper in the web interface. # If this options is not defined, the hostname is used. #name: # The tags of the shipper are included in their own field with each # transaction published. Tags make it easy to group servers by different # logical properties. #tags: ["service-X", "web-tier"] # Uncomment the following if you want to ignore transactions created # by the server on which the shipper is installed. This option is useful # to remove duplicates if shippers are installed on multiple servers. #ignore_outgoing: true # How often (in seconds) shippers are publishing their IPs to the topology map. # The default is 10 seconds. #refresh_topology_freq: 10 # Expiration time (in seconds) of the IPs published by a shipper to the topology map. # All the IPs will be deleted afterwards. Note, that the value must be higher than # refresh_topology_freq. The default is 15 seconds. #topology_expire: 15 # Internal queue size for single events in processing pipeline #queue_size: 1000 # Configure local GeoIP database support. # If no paths are not configured geoip is disabled. #geoip: #paths: # - "/usr/share/GeoIP/GeoLiteCity.dat" # - "/usr/local/var/GeoIP/GeoLiteCity.dat" ############################# Logging ######################################### # There are three options for the log ouput: syslog, file, stderr. # Under Windos systems, the log files are per default sent to the file output, # under all other system per default to syslog. # 建议在开发时期开启日志并把日志调整为debug或者info级别，在生产环境下调整为error级别 # 开启日志 必须设置to_files 属性为true logging: # Send all logging output to syslog. On Windows default is false, otherwise # default is true. # 配置beats日志。日志可以写入到syslog也可以是轮滚日志文件。默认是syslog # tail -f /var/log/messages #to_syslog: true # Write all logging output to files. Beats automatically rotate files if rotateeverybytes # limit is reached. # 日志发送到轮滚文件 #to_files: false # To enable logging to files, to_files option has to be set to true # to_files设置为true才可以开启轮滚日志记录 files: # The directory where the log files will written to. # 指定日志路径 #path: /var/log/mybeat # The name of the files where the logs are written to. # 指定日志名称 #name: mybeat # Configure log file size limit. If limit is reached, log file will be # automatically rotated # 默认文件达到10M就会滚动生成新文件 rotateeverybytes: 10485760 # = 10MB # Number of rotated log files to keep. Oldest files will be deleted first. # 保留日志文件周期。 默认 7天。值范围为2 到 1024 #keepfiles: 7 # Enable debug output for selected components. To enable all selectors use ["*"] # Other available selectors are beat, publish, service # Multiple selectors can be chained. #selectors: [ ] # Sets log level. The default log level is error. # Available log levels are: critical, error, warning, info, debug # 日志级别，默认是error #level: error  
```  

## [运用Filebeat module分析nginx日志](https://blog.csdn.net/UbuntuTouch/article/details/101604717)  

## [ELK--filebeat详解](https://www.cnblogs.com/kuku0223/p/8316922.html)  

## [Filebeat 模块与配置](https://www.cnblogs.com/cjsblog/p/9495024.html)  



# logstash  

## Logstash使用grok过滤nginx日志  

　　在生产环境中，nginx日志格式往往使用的是自定义的格式，我们需要把logstash中的message结构化后再存储，方便kibana的搜索和统计，因此需要对message进行解析。  

　　本文采用grok过滤器，使用match正则表达式解析，根据自己的log_format定制。  

1、nginx日志格式  

　　log_format配置如下：  

```  
log_format  main  '$remote_addr - $remote_user [$time_local] $http_host $request_method "$uri" "$query_string" '  
                  '$status $body_bytes_sent "$http_referer" $upstream_status $upstream_addr $request_time $upstream_response_time '  
                  '"$http_user_agent" "$http_x_forwarded_for"' ;  
```  

　　对应的日志如下：  

```  
1.1.1.1 - - [06/Jun/2016:00:00:01 +0800] www.test.com GET "/api/index" "?cms=0&rnd=1692442321" 200 4 "http://www.test.com/?cp=sfwefsc" 200 192.168.0.122:80 0.004 0.004 "Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.101 Safari/537.36" "-"  
```  

2、编写正则表达式  

　　logstash中默认存在一部分正则让我们来使用，可以访问[Grok Debugger](https://grokdebug.herokuapp.com/patterns)来查看，可以在 $logstash/vendor/bundle/jruby/1.9/gems/logstash-patterns-core-4.0.0/patterns/ 目录里面查看。  

　　基本定义在grok-patterns中，我们可以使用其中的正则，当然并不是所有的都适合nginx字段，这时就需要我们自定义正则，然后通过指定patterns_dir来调用。  

　　同时在写正则的时候可以使用[Grok Debugger](https://grokdebug.herokuapp.com/)或者[Grok Comstructor](http://grokconstructor.appspot.com/do/match)工具来帮助我们更快的调试。在不知道如何使用logstash中的正则的时候也可使用Grok Debugger的Descover来自动匹配。  

　　1）nginx标准日志格式  

　　　　logstash自带的grok正则中有Apache的标准日志格式：  

```  
COMMONAPACHELOG %{IPORHOST:clientip} %{HTTPDUSER:ident} %{USER:auth} \[%{HTTPDATE:timestamp}\] "(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})" %{NUMBER:response} (?:%{NUMBER:bytes}|-)  
COMBINEDAPACHELOG %{COMMONAPACHELOG} %{QS:referrer} %{QS:agent}  
```  

　　　　对于nginx标准日志格式，可以发现只是最后多了一个 $http_x_forwarded_for 变量。则nginx标准日志的grok正则定义为：  

```  
MAINNGINXLOG %{COMBINEDAPACHELOG} %{QS:x_forwarded_for}  
```  

　　2）自定义格式  

　　　　通过log_format来匹配对应的正则如下：  

```  
%{IPV4:remote_addr} - (%{USERNAME:user}|-) \[%{HTTPDATE:log_timestamp}\] (%{HOSTNAME1:http_host}|-) (%{WORD:request_method}|-) \"(%{URIPATH1:uri}|-|)\" \"(%{URIPARM1:param}|-)\" %{STATUS:http_status} (?:%{BASE10NUM:body_bytes_sent}|-) \"(?:%{GREEDYDATA:http_referrer}|-)\" (%{STATUS:upstream_status}|-) (?:%{HOSTPORT1:upstream_addr}|-) (%{BASE16FLOAT:upstream_response_time}|-) (%{STATUS:request_time}|-) \"(%{GREEDYDATA:user_agent}|-)\" \"(%{FORWORD:x_forword_for}|-)\"  
```  

　　　　这里面有几个是我自定义的正则：  

```  
URIPARM1 [A-Za-z0-9$.+!*'|(){},~@#%&/=:;^\\_<>`?\-\[\]]*  
URIPATH1 (?:/[\\A-Za-z0-9$.+!*'(){},~:;=@#% \[\]_<>^\-&?]*)+  
HOSTNAME1 \b(?:[0-9A-Za-z_\-][0-9A-Za-z-_\-]{0,62})(?:\.(?:[0-9A-Za-z_\-][0-9A-Za-z-:\-_]{0,62}))*(\.?|\b)  
STATUS ([0-9.]{0,3}[, ]{0,2})+  
HOSTPORT1 (%{IPV4}:%{POSINT}[, ]{0,2})+  
FORWORD (?:%{IPV4}[,]?[ ]?)+|%{WORD}  
```  

　　message是每段读进来的日志，IPORHOST、USERNAME、HTTPDATE等都是patterns/grok-patterns中定义好的正则格式名称，对照日志进行编写。  

　　grok pattren的语法为：%{SYNTAX:semantic}，":" 前面是grok-pattrens中定义的变量，后面可以自定义变量的名称。(?:%{SYNTAX:semantic}|-)这种形式是条件判断。  

　　如果有双引号""或者中括号[]，需要加 \ 进行转义。  

　　详解自定义正则：  

 URIPARAM \?[A-Za-z0-9$.+!*'|(){},~@#%&/=:;_?\-\[\]<>]*  

 URIPARM1 [A-Za-z0-9$.+!*'|(){},~@#%&/=:;^\\_<>`?\-\[\]]* grok-patterns中正则表达式，可以看到grok-patterns中是以“?”开始的参数，在nginx的 $query_string 中已经把“?”去掉了，所以我们这里不再需要“?”。另外单独加入日志中出现的  ^ \ _ < > ` 特殊符号  

 URIPATH (?:/[A-Za-z0-9$.+!*'(){},~:;=@#%&_\-]*)+  

 URIPATH1 (?:/[\\A-Za-z0-9$.+!*'(){},~:;=@#% \[\]_<>^\-&?]*)+ grok-patterns中正则表达式，grok-patterns中的URIPATH不能匹配带空格的URI，于是在中间加一个空格。另外还有 \ [ ] < > ^ 特殊符号。  

 HOSTNAME \b(?:[0-9A-Za-z][0-9A-Za-z-]{0,62})(?:\.(?:[0-9A-Za-z][0-9A-Za-z-]{0,62}))*(\.?|\b)  

 HOSTNAME1 \b(?:[0-9A-Za-z_\-][0-9A-Za-z-_\-]{0,62})(?:\.(?:[0-9A-Za-z_\-][0-9A-Za-z-:\-_]{0,62}))*(\.?|\b) 添加匹配 http_host 中带有 "-" 的字符。  

 HOSTPORT %{IPORHOST}:%{POSINT}  

 HOSTPORT1 (%{IPV4}:%{POSINT}[, ]{0,2})+ 在匹配 upstream_addr 字段时发现，会出现多个IP地址的情况出现，匹配多个IP地址。  

 STATUS ([0-9.]{0,3}[, ]{0,2})+ 该字段是当出现多个 upstream_addr 字段时匹配多个 http_status 。  

 FORWORD (?:%{IPV4}[,]?[ ]?)+|%{WORD} 当 x_forword_for 字段出现多个IP地址时匹配。  

　　nginx左右字段都定义完成，可以使用[Grok Debugger](https://grokdebug.herokuapp.com/)或者[Grok Comstructor](http://grokconstructor.appspot.com/do/match)工具来测试。添加自定义正则的时候，在Grok Debugger中可以勾选“Add custom patterns”。  

　　以上日志匹配结果为：  

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)  

```  
{  
  "remote_addr": [  
    "1.1.1.1"  
  ],  
  "user": [  
    "-"  
  ],  
  "log_timestamp": [  
    "06/Jun/2016:00:00:01 +0800"  
  ],  
  "http_host": [  
    "www.test.com"  
  ],  
  "request_method": [  
    "GET"  
  ],  
  "uri": [  
    "/api/index"  
  ],  
  "param": [  
    "?cms=0&rnd=1692442321"  
  ],  
  "http_status": [  
    "200"  
  ],  
  "body_bytes_sent": [  
    "4"  
  ],  
  "http_referrer": [  
    "http://www.test.com/?cp=sfwefsc"  
  ],  
  "port": [  
    null  
  ],  
  "upstream_status": [  
    "200"  
  ],  
  "upstream_addr": [  
    "192.168.0.122:80"  
  ],  
  "upstream_response_time": [  
    "0.004"  
  ],  
  "request_time": [  
    "0.004"  
  ],  
  "user_agent": [  
    ""Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.101 Safari/537.36""  
  ],  
  "client_ip": [  
    "2.2.2.2"  
  ],  
  "x_forword_for": [  
    null  
  ]  
}  
```  

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)  

3、logstash的配置文件  

　　创建自定义正则目录  

```  
# mkdir -p /usr/local/logstash/patterns  
# vi /usr/local/logstash/patterns/nginx  
```  

　　然后写入上面自定义的正则  

```  
URIPARM1 [A-Za-z0-9$.+!*'|(){},~@#%&/=:;_?\-\[\]]*  
URIPATH1 (?:/[A-Za-z0-9$.+!*'(){},~:;=@#%&_\- ]*)+  
URI1 (%{URIPROTO}://)?(?:%{USER}(?::[^@]*)?@)?(?:%{URIHOST})?(?:%{URIPATHPARAM})?  
NGINXACCESS %{IPORHOST:remote_addr} - (%{USERNAME:user}|-) \[%{HTTPDATE:log_timestamp}\] %{HOSTNAME:http_host} %{WORD:request_method} \"%{URIPATH1:uri}\" \"%{URIPARM1:param}\" %{BASE10NUM:http_status} (?:%{BASE10NUM:body_bytes_sent}|-) \"(?:%{URI1:http_referrer}|-)\" (%{BASE10NUM:upstream_status}|-) (?:%{HOSTPORT:upstream_addr}|-) (%{BASE16FLOAT:upstream_response_time}|-) (%{BASE16FLOAT:request_time}|-) (?:%{QUOTEDSTRING:user_agent}|-) \"(%{IPV4:client_ip}|-)\" \"(%{WORD:x_forword_for}|-)\"  
```  

　　logstash.conf配置文件内容  

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)  

```  
input {  
        file {  
                path => "/data/nginx/logs/access.log"  
                type => "nginx-access"  
                start_position => "beginning"  
                sincedb_path => "/usr/local/logstash/sincedb"  
        }  
}  
filter {  
        if [type] == "nginx-access" {  
                grok {  
                        patterns_dir => "/usr/local/logstash/patterns"        //设置自定义正则路径  
                        match => {  
                                "message" => "%{NGINXACCESS}"  
                        }  
                }  
                date {  
                        match => [ "log_timestamp" , "dd/MMM/YYYY:HH:mm:ss Z" ]  
                }  
　　　　　　　　　urldecode {  
        　　　　　　　　　all_fields => true  
    　　　　　　　}  
　　　　　　　　　//把所有字段进行urldecode（显示中文）  
        }  
}  
output {  
        if [type] == "nginx-access" {  
                elasticsearch {  
                        hosts => ["10.10.10.26:9200"]  
                        manage_template => true  
                        index => "logstash-nginx-access-%{+YYYY-MM}"  
                }  
        }  

}  
```  

[![复制代码](https://common.cnblogs.com/images/copycode.gif)](javascript:void(0);)  

 

 

 4、启动logstash，然后就可以查看日志是否写入elasticsearch中。  



参考：[Logstash使用grok过滤nginx日志](https://www.cnblogs.com/Orgliny/p/5592186.html)  


## 配置文件自动重载工作原理  

-  检测到配置文件变化  
- 通过停止所有输入停止当前pipline  
- 用新的配置创建一个新的管道  
- 检查配置文件语法是否正确  
- 检查所有的输入和输出是否可以初始化  
- 检查成功使用新的pipeline替换当前的pipeline,  
- 检查失败,使用旧的继续工作.  
- 在重载过程中,jvm没有重启.  

## logstash日志记录sincedb原理  

由于测试玩耍，读取日志文件之后就不重新读取，需要每次把sincedb文件删除了，才会重新读取日志文件  
低版本的Logstash2.4.0默认的sincedb文件目录在home目录下，通过ll -a就可以查看到  
但是：Logstash6.5.4之后在home目录下没有找到  
使用：find / -name .sincedb_*  
查看到sincedb文件在/var/lib/logstash/plugins/inputs/file/.sincedb_d2343edad78a7252d2ea9cba15bbff6d  
删除就能重新读取日志文件了，也可以指定指定sincedb文件的目录使用，重要该路径必须指定到文件不能指定到文件的目录  
sincedb_path => "/home/logs/index.txt"  
还有一种方式使用：利用linux黑洞可以达到每次重头读取日志文件  
sincedb_path => "/dev/null"  

如果Logstash重启，对于同一个文件，会继续从上次记录的位置开始读取。  

## 使用Logstash的grok过滤日志文件  

可以使用Logstash的grok模块对任意文本解析并结构化输出。Logstash默认带有120中匹配模式。  

可以参见源代码  

logstash/patterns/grok-patterns  

logstash/lib/logstash/filters/grok.rb  



grok的语法格式为 %{SYNTAX:SEMANTIC}  

SYNTAX是文本要匹配的模式，例如3.14匹配 NUMBER 模式，127.0.0.1 匹配 IP 模式。  



SEMANTIC 是匹配到的文本片段的标识。例如 “3.14” 可以是一个时间的持续时间，所以可以简单地叫做"duration" ，字符串"55.3.244.1"可以被标识为“client”  

所以，grok过滤器表达式可以写成：  



%{NUMBER:duration} %{IP:client}  



默认情况下，所有的SEMANTIC是以字符串的方式保存，如果想要转换一个SEMANTIC的数据类型，例如转换一个字符串为×××，可以写成如下的方式：  



%{NUMBER:num:int}  



例如日志  

55.3.244.1 GET /index.html 15824 0.043  



可以写成如下的grok过滤表达式  



%{IP:client} %{WORD:method} %{URIPATHPARAM:request} %{NUMBER:bytes} %{NUMBER:duration}  

再举一个实际的案例  

常规的Apache日志  

```plain  
127.0.0.1 - - [13/Apr/2015:17:22:03 +0800] "GET /router.php HTTP/1.1" 404 285 "-" "curl/7.19.7 (x86_64-redhat-linux-gnu) libcurl/7.19.7 NSS/3.15.3 zlib/1.2.3 libidn/1.18 libssh2/1.4.2"  
127.0.0.1 - - [13/Apr/2015:17:22:03 +0800] "GET /router.php HTTP/1.1" 404 285 "-" "curl/7.19.7 (x86_64-redhat-linux-gnu) libcurl/7.19.7 NSS/3.15.3 zlib/1.2.3 libidn/1.18 libssh2/1.4.2"  
```  

使用Logstash收集  

```plain  
 input{  
 
  file {  
    type => "apache"  
    path => "/var/log/httpd/access_log"  
    exclude => ["*.gz"]  
    sincedb_path => "/dev/null"  
 
       }  
 
      }  

output {  
   stdout {  
      codec => rubydebug  
          }  
        }  
```  

显示：  

```plain  
{  
       "message" => "127.0.0.1 - - [13/Apr/2015:17:22:03 +0800] \"GET /router.php HTTP/1.1\" 404 285 \"-\" \"curl/7.19.7 (x86_64-redhat-linux-gnu) libcurl/7.19.7 NSS/3.15.3 zlib/1.2.3 libidn/1.18 libssh2/1.4.2\"",  
      "@version" => "1",  
    "@timestamp" => "2015-04-13T09:22:03.844Z",  
          "type" => "apache",  
          "host" => "xxxxxx",  
          "path" => "/var/log/httpd/access_log"  
}  
{  
       "message" => "127.0.0.1 - - [13/Apr/2015:17:22:03 +0800] \"GET /router.php HTTP/1.1\" 404 285 \"-\" \"curl/7.19.7 (x86_64-redhat-linux-gnu) libcurl/7.19.7 NSS/3.15.3 zlib/1.2.3 libidn/1.18 libssh2/1.4.2\"",  
      "@version" => "1",  
    "@timestamp" => "2015-04-13T09:22:03.844Z",  
          "type" => "apache",  
          "host" => "xxxxxx",  
          "path" => "/var/log/httpd/access_log"  
}  
```  

修改配置如下：  





```plain  
input {  

  file {  
    type => "apache"  
    path => "/var/log/httpd/access_log"  
    exclude => ["*.gz"]  
    sincedb_path => "/dev/null"  
    
       }  

      }  
filter {  
  if [type] == "apache" {  
     grok {  
          match => ["message",  "%{COMBINEDAPACHELOG}"]  
          }  
                         }  
       }  

output {  
   stdout {  
      codec => rubydebug  
          }  
       }  
```  



显示：  

```plain  
{  
        "message" => "127.0.0.1 - - [14/Apr/2015:09:53:40 +0800] \"GET /router.php HTTP/1.1\" 404 285 \"-\" \"curl/7.19.7 (x86_64-redhat-linux-gnu) libcurl/7.19.7 NSS/3.15.3 zlib/1.2.3 libidn/1.18 libssh2/1.4.2\"",  
       "@version" => "1",  
     "@timestamp" => "2015-04-14T01:53:57.182Z",  
           "type" => "apache",  
           "host" => "xxxxxxxx",  
           "path" => "/var/log/httpd/access_log",  
       "clientip" => "127.0.0.1",  
          "ident" => "-",  
           "auth" => "-",  
      "timestamp" => "14/Apr/2015:09:53:40 +0800",  
           "verb" => "GET",  
        "request" => "/router.php",  
    "httpversion" => "1.1",  
       "response" => "404",  
          "bytes" => "285",  
       "referrer" => "\"-\"",  
          "agent" => "\"curl/7.19.7 (x86_64-redhat-linux-gnu) libcurl/7.19.7 NSS/3.15.3 zlib/1.2.3 libidn/1.18 libssh2/1.4.2\""  
}  
{  
        "message" => "127.0.0.1 - - [14/Apr/2015:09:53:40 +0800] \"GET /router.php HTTP/1.1\" 404 285 \"-\" \"curl/7.19.7 (x86_64-redhat-linux-gnu) libcurl/7.19.7 NSS/3.15.3 zlib/1.2.3 libidn/1.18 libssh2/1.4.2\"",  
       "@version" => "1",  
     "@timestamp" => "2015-04-14T01:53:57.187Z",  
           "type" => "apache",  
           "host" => "xxxxxxx",  
           "path" => "/var/log/httpd/access_log",  
       "clientip" => "127.0.0.1",  
          "ident" => "-",  
           "auth" => "-",  
      "timestamp" => "14/Apr/2015:09:53:40 +0800",  
           "verb" => "GET",  
        "request" => "/router.php",  
    "httpversion" => "1.1",  
       "response" => "404",  
          "bytes" => "285",  
       "referrer" => "\"-\"",  
          "agent" => "\"curl/7.19.7 (x86_64-redhat-linux-gnu) libcurl/7.19.7 NSS/3.15.3 zlib/1.2.3 libidn/1.18 libssh2/1.4.2\""  
}  
```  



这里的%{COMBINEDAPACHELOG} 是logstash自带的匹配模式  

more /usr/share/logstash/vendor/bundle/jruby/2.5.0/gems/logstash-patterns-core-4.1.2/patterns/grok-patterns  



```plain  
COMMONAPACHELOG %{IPORHOST:clientip} %{USER:ident} %{USER:auth} \[%{HTTPDATE:timestamp}\] "(?:%{WORD:verb} %{NOTSPACE:req  
uest}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})" %{NUMBER:response} (?:%{NUMBER:bytes}|-)  
COMBINEDAPACHELOG %{COMMONAPACHELOG} %{QS:referrer} %{QS:agent}  
```  



grok可以支持任意正则表达式  

所以支持的正则表达式的语法可以参见  

http://www.geocities.jp/kosako3/oniguruma/doc/RE.txt  



在有些情况下自带的匹配模式无法满足需求，可以自定义一些匹配模式  

首先可以根据正则表达式匹配文本片段  



(?<field_name>the pattern here)  



例如，postfix日志有一个字段表示 queue id，可以使用以下表达式进行匹配：  



(?<queue_id>[0-9A-F]{10,11}  



可以手动创建一个匹配文件  

\#   # contents of ./patterns/postfix:  

   POSTFIX_QUEUEID [0-9A-F]{10,11}  

```plain  
    Jan  1 06:25:43 mailserver14 postfix/cleanup[21403]: BEF25A72965: message-id=<20130101142543.5828399CCAF@mailserver14.example.com>  

     filter {  
       grok {  
         patterns_dir => "./patterns"  
         match => [ "message", "%{SYSLOGBASE} %{POSTFIX_QUEUEID:queue_id}: %{GREEDYDATA:syslog_message}" ]  
       }  
     }  

 The above will match and result in the following fields:  

 * timestamp: Jan  1 06:25:43  
 * logsource: mailserver14  
 * program: postfix/cleanup  
 * pid: 21403  
 * queue_id: BEF25A72965  
 * syslog_message: message-id=<20130101142543.5828399CCAF@mailserver14.example.com>  

 The `timestamp`, `logsource`, `program`, and `pid` fields come from the  
 SYSLOGBASE pattern which itself is defined by other patterns.  
```  



可以使用重写  

```plain  
The fields to overwrite.  
 
 This allows you to overwrite a value in a field that already exists.  
 
   For example, if you have a syslog line in the 'message' field, you can  
   overwrite the 'message' field with part of the match like so:  
  
       filter {  
         grok {  
           match => [  
             "message",  
             "%{SYSLOGBASE} %{DATA:message}"  
           ]  
           overwrite => [ "message" ]  
         }  
       }  
  
    In this case, a line like "May 29 16:37:11 sadness logger: hello world"  
    will be parsed and 'hello world' will overwrite the original message.  
```  

参考文档：  

http://logstash.net/docs/1.4.2/filters/grok  

https://github.com/logstash/logstash/tree/v1.4.2/patterns  

## 修改Logstash的@timestamp字段为业务时间  

Logstash在处理数据的时候，会自动生成一个字段`@timestamp`，默认该字段存储的是Logstash收到消息/事件(event)的时间。很多时候我们用ELK是处理日志的，日志里面一般都是有时间的。而且很多时候我们只关注日志里面的时间，而不关注Logstash收到这条日志的时间。这个时候，一种方法是再增加一个字段，用来存储日志里面的时间，这种很简单；另一种方法是使用日志中的时间替换掉`@timestamp`字段默认的时间。本文介绍第二种方法并总结一些关键知识点。  

现在有如下一条日志：  

```  
2018-02-26 15:48:32.708-[INFO ] main RestfulApiProvider - initializing restful api provider  
```  

我们先用最简单的Logstash规则解析，规则文件**test.conf**如下：  

```  
# 从标准输入读入数据  
input { stdin {} }            

# 只解析时间戳，其它信息不管  
filter {  
    grok {  
        match => {  "message" => "(?<timestamp>%{TIMESTAMP_ISO8601})"  }  
    }  
}  

# 输出到标准输出  
output {  
 stdout { codec => rubydebug }  
}  
```  

启动Logstash：`bin/logstash -f test.conf`  

在标准输入粘贴上面的日志并回车，输出如下：  

```  
{  
    "host" => "NiYanchuns-MacBook-Air.local",  
    "@timestamp" => 2018-10-18T12:20:51.603Z,  
    "timestamp" => "2018-02-26 15:48:32.708",  
    "message" => "2018-02-26 15:48:32.708-[INFO ] main RestfulApiProvider - initializing restful api provider",  
    "@version" => "1"  
}  
```  

这里需要注意以下几个点：  

- 解析规则里面的grok是Logstash的一个正则解析插件，Logstash强大的解析能力主要来自于它，使用文档看[这里](https://www.elastic.co/guide/en/logstash/current/plugins-filters-grok.html)。  
- `@timestamp`字段是内置的，和之前说的一样，时间是Logstash收到消息的时间，而且注意**使用的是UTC时间**，我电脑的时间是北京时间晚上八点多。  
- `timestamp`字段是我们解析规则里面定义的字段（字段名随便起，不能有特殊符号，比如`@`），该字段存储的是从日志里面解析出来的时间，注意这个时间格式完全是日志里面时间的格式。要注意我们加的`timestamp`和系统内置的`@timestamp`不是同一个字段。  
- `message`：也是程序内置字段，内容为消息原始内容。  

既然如此，那我们能不能通过自定义一个与程序内置的`@timestamp`同名的字段来覆盖掉程序内置字段呢？比如将上面解析规则里面的`timestamp`改为`@timestamp`。  

答案是**不行**。刚才已经说了，系统内置字段名前可以加`@`，但我们定义字段的时候字段名不能加`@`，也就是说我们不能通过定义一个`@timestamp`字段覆盖掉系统默认的，那样配置语法检查就通不过。有(bu)兴(xin)趣(xie)的可以试一下。  

那如何做呢？Logstash提供了一个**Date**插件可以实现该功能，使用文档见[这里](https://www.elastic.co/guide/en/logstash/current/plugins-filters-date.html)。我们修改一下刚才的规则：  

```  
input { stdin {} }  

filter {  
    grok {  
        match => {  "message" => "(?<timestamp>%{TIMESTAMP_ISO8601})"  }  
    }  
    date {  
        match => [ "timestamp", "ISO8601" ]  
    }  
}  

output {  
 stdout { codec => rubydebug }  
}  
```  

重启Logstash后，输入日志后，解析输出如下：  

```  
{  
    "@version" => "1",  
    "host" => "NiYanchuns-MacBook-Air.local",  
    "timestamp" => "2018-02-26 15:48:32.708",  
    "@timestamp" => 2018-02-26T07:48:32.708Z,  
    "message" => "2018-02-26 15:48:32.708-[INFO ] main RestfulApiProvider - initializing restful api provider"  
}  
```  

OK，`@timestamp`字段里面的值已经和`timestamp`字段很像了，但不完全一样。Logstash将`timestamp`的时间根据系统的时区转换为UTC时间存到了`@timestamp`字段里面。  

这个时候`timestamp`就成多余的了，我们可以通过`mutate`插件移除该字段。再次修改解析规则文件：  

```  
input { stdin {} }  

filter {  
    grok {  
        match => {  "message" => "(?<timestamp>%{TIMESTAMP_ISO8601})"  }  
    }  
    date {  
        match => [ "timestamp", "ISO8601" ]  
    }  
    mutate {  
         remove_field => [ "ts_tmp","date", "time", "end_time_tmp", "end_time_tmp1" ]  
    }  
}  

output {  
 stdout { codec => rubydebug }  
}  
```  

重启Logstash后，输入日志后，解析输出如下：  

```  
{  
    "message" => "2018-02-26 15:48:32.708-[INFO ] main RestfulApiProvider - initializing restful api provider",  
    "@version" => "1",  
    "@timestamp" => 2018-02-26T07:48:32.708Z,  
    "host" => "NiYanchuns-MacBook-Air.local"  
}  
```  

`timestamp`字段已经在输出中去掉了。  

我们简单介绍一下`Date`插件。Date常见的配置如下：  

```  
date {  
    match => [ "time_field", "yyyyMMdd HH:mm:ss.SSS" ]  
    # timezone => "UTC"  
    target => "end_time"  
}  
```  

上述配置的含义是，将`time_field`字段按照`yyyyMMdd HH:mm:ss.SSS`格式解析后存到target指定的字段`end_time`字段去。`time_field`必须是已经定义的字段，最常见的就是在grok里面解析出来的某个时间字段。时间格式可查看Date插件的文档。如果没有指定target，默认就是`@timestamp`字段，这就是为什么我们可以使用该插件来修改`@timestamp`字段值的原因。  

另外，`timezone`字段在某些场景下也非常重要，如果从时间的值里面解析不出来时区，而且我们也没有指定时区的话，程序就会认为我们的时间字段的时区就是系统所处时区。比如上面从`timestamp`转到`@timestamp`的时候，时间值里面没有时区，所以使用了系统的时区东八区。当然，我们可以使用该字段指定时区。  

OK，本文就到这里，干(hong)正(wo)事(wa)去了。  

**参考:**  

[修改Logstash的@timestamp字段为业务时间](https://niyanchun.com/modify-attimestamp-field-in-logstash.html)  

[使用kibana分析nginx日志](https://blog.csdn.net/weixin_43342753/article/details/89340022)  

## logstash @timestamp时间时区的问题  

  最近使用logstash的作为日志处理工具是发现一个问题,logstash给提供了一个默认的时间字段@timestamp,这个时间无论我怎么该他都是０时区的时间，没有办法改成+08:00时区的时间，后来查了很多资料发现原来是代码直接获取的UTC默认的时间，所以无论怎么更改系统时间它都不会改变。这对于我司运维很痛苦，像我提了多次需求。  

  后来终于找到了修改的办法，在filter里面加上这段代码即可修改。  

  

ruby {  

​    code => "event.timestamp.time.localtime"  

  }  

  加上以后再看@timestamp就是我所需要的系统时间。  





  

# grok  

## [grok语法验证](http://grokdebug.herokuapp.com/)  

## grok模式参考  

参考：  

https://help.aliyun.com/document_detail/135045.html?spm=5176.11065259.1996646101.searchclickresult.6c9338bfo4HTnK  

GROK是一种采用组合多个预定义的正则表达式，用来匹配分割文本并映射到关键字的工具。通常用来对日志数据进行处理。本文档主要介绍GROK的模式说明以及常用语法。  

GROK模式及说明如下表所示。  

| 类型               | 模式（pattern）                                              | 说明                                                |  
| :----------------- | :----------------------------------------------------------- | :-------------------------------------------------- |  
| 常见模式           | CHINAID                                                      | 匹配中国居民身份证号。                              |  
| USERNAME           | 匹配字母、数字和`._-`组合。                                  |                                                     |  
| USER               | 匹配字母、数字和`._-`组合。                                  |                                                     |  
| EMAILLOCALPART     | 匹配邮箱从开头到@字符前内容，如123456@alibaba.com，匹配内容为123456。 |                                                     |  
| EMAILADDRESS       | 匹配邮箱。                                                   |                                                     |  
| HTTPDUSER          | 匹配邮箱或者用户名。                                         |                                                     |  
| INT                | 匹配INT数字。                                                |                                                     |  
| BASE10NUM          | 匹配十进制数。                                               |                                                     |  
| NUMBER             | 匹配数字。                                                   |                                                     |  
| BASE16NUM          | 匹配十六进制数。                                             |                                                     |  
| BASE16FLOAT        | 匹配十六进制浮点数。                                         |                                                     |  
| POSINT             | 匹配正整数。                                                 |                                                     |  
| NONNEGINT          | 匹配非负整数。                                               |                                                     |  
| WORD               | 匹配字母或一句话。                                           |                                                     |  
| NOTSPACE           | 匹配非空格内容。                                             |                                                     |  
| SPACE              | 匹配空格。                                                   |                                                     |  
| DATA               | 匹配换行符。                                                 |                                                     |  
| GREEDYDATA         | 匹配0个或多个除换行符。                                      |                                                     |  
| QUOTEDSTRING       | 匹配引用内容如：`I am "Iron Man"`，会匹配`Iron Man`内容。    |                                                     |  
| UUID               | 匹配UUID。                                                   |                                                     |  
| Networking         | MAC                                                          | 匹配MAC地址。                                       |  
| CISCOMAC           | 匹配CISCOMAC地址。                                           |                                                     |  
| WINDOWSMAC         | 匹配WINDOWSMAC地址。                                         |                                                     |  
| COMMONMAC          | 匹配COMMONMAC地址。                                          |                                                     |  
| IPV6               | 匹配IPV6。                                                   |                                                     |  
| IPV4               | 匹配IPV4。                                                   |                                                     |  
| IP                 | 匹配IPV6或IPV4。                                             |                                                     |  
| HOSTNAME           | 匹配HOSTNAME。                                               |                                                     |  
| IPORHOST           | 匹配IP或HOSTNAME。                                           |                                                     |  
| HOSTPORT           | 匹配IPORHOST或者POSTINT。                                    |                                                     |  
| Paths              | PATH                                                         | 匹配UNIXPATH或者WINPATH。                           |  
| UNIXPATH           | 匹配UNIXPATH。                                               |                                                     |  
| WINPATH            | 匹配WINPATH。                                                |                                                     |  
| URIPROTO           | 匹配URI中的头部分，如`http://hostname.domain.tld/_astats?application=&inf.name=eth0`会匹配到`http`。 |                                                     |  
| TTY                | 匹配TTY路径。                                                |                                                     |  
| URIHOST            | 匹配IPORHOST和POSINT。还是以`http://hostname.domain.tld/_astats?application=&inf.name=eth0`为例，会匹配到`hostname.domain.tld`。 |                                                     |  
| URI                | 匹配内容中的URI。                                            |                                                     |  
| 日期               | MONTH                                                        | 匹配数字或者月份英文缩写或者全拼等格式月份。        |  
| MONTHNUM           | 匹配数字格式月份。                                           |                                                     |  
| MONTHDAY           | 匹配月份中的day。                                            |                                                     |  
| DAY                | 匹配星期的英文全拼或者缩写。                                 |                                                     |  
| YEAR               | 匹配年份。                                                   |                                                     |  
| 时间               | HOUR                                                         | 匹配小时。                                          |  
| MINUTE             | 匹配分钟。                                                   |                                                     |  
| SECOND             | 匹配秒。                                                     |                                                     |  
| TIME               | 匹配完整的时间。                                             |                                                     |  
| DATE_US            | 匹配MonthDay-Year链接符，也可以是`/`组合形式的日期。         |                                                     |  
| DATE_EU            | 匹配MonthDay-Year链接符，也可以是`/`或者`.`组合形式的日期。  |                                                     |  
| ISO8601_TIMEZONE   | 匹配ISO8601格式的Hour和Minute。                              |                                                     |  
| ISO8601_SECOND     | 匹配ISO8601格式的Second。                                    |                                                     |  
| TIMESTAMP_ISO8601  | 匹配ISO8601格式的Time。                                      |                                                     |  
| DATE               | 匹配US或EU格式的时间。                                       |                                                     |  
| DATESTAMP          | 匹配完整日期和时间。                                         |                                                     |  
| TZ                 | 匹配UTC。                                                    |                                                     |  
| DATESTAMP_RFC822   | 匹配RFC822格式时间。                                         |                                                     |  
| DATESTAMP_RFC2822  | 匹配RFC2822格式时间。                                        |                                                     |  
| DATESTAMP_OTHER    | 匹配其他格式时间。                                           |                                                     |  
| DATESTAMP_EVENTLOG | 匹配EVENTLOG格式时间。                                       |                                                     |  
| HTTPDERROR_DATE    | 匹配HTTPDERROR格式时间。                                     |                                                     |  
| SYSLOG             | SYSLOGTIMESTAMP                                              | 匹配Syslog格式时间。                                |  
| PROG               | 匹配program内容。                                            |                                                     |  
| SYSLOGPROG         | 匹配program和pid内容。                                       |                                                     |  
| SYSLOGHOST         | 匹配IPORHOST。                                               |                                                     |  
| SYSLOGFACILITY     | 匹配facility。                                               |                                                     |  
| HTTPDATE           | 匹配日期时间。                                               |                                                     |  
| LOGFORMATL         | LOGFORMAT                                                    | 匹配Syslog，默认TraditionalFormat格式的Syslog日志。 |  
| COMMONAPACHELOG    | 匹配commonApache日志。                                       |                                                     |  
| COMBINEDAPACHELOG  | 匹配组合Apache日志。                                         |                                                     |  
| HTTPD20_ERRORLOG   | 匹配HTTPD20日志。                                            |                                                     |  
| HTTPD24_ERRORLOG   | 匹配HTTPD24日志。                                            |                                                     |  
| HTTPD_ERRORLOG     | 匹配HTTPD日志。                                              |                                                     |  
| LOGLEVELS          | LOGLEVELS                                                    | 匹配Log的Level。例如warn，debug等。                 |  

## grok采集nginx日志  

首先规范nginx日志，之前的文章已经实现了，链接：  

[nginx日志规范](../_posts/2020-04-02-nginx日志规范.md)  

```shell  
log_format  main  '$remote_addr:$remote_port $remote_user [$time_local] "$request_method $scheme://$http_host$request_uri" $status $request_time '  
                  '$request_length $body_bytes_sent "$http_referer" "$http_user_agent" '  
                  '$upstream_addr $upstream_status $upstream_response_time $http_x_forwarded_for';  

58.56.27.130:50429 - [02/Apr/2020:09:26:32 +0800] "GET http://thxa.52wandoumiao.cn/hxan/index/getAppMenu?roles=5&safeRoles=2" 200 0.027 552 145 "-" "Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36" 172.19.2.50:8087 200 0.027 -  

match => { "message" => "%{IPV4:remote_ip}:%{POSINT:remote_port} (%{USERNAME:user}|-) \[%{HTTPDATE:timestamp}\] \"(?:%{WORD:http_method} %{WORD:scheme}://%{IPORHOST:http_host}%{NOTSPACE:request_uri}(?: HTTP/%{NUMBER:httpvers  
ion})?)\" (%{NUMBER:http_status}) (%{NUMBER:request_time}) (%{NUMBER:request_length}|-) (%{NUMBER:body_bytes_sent}|-) (%{QUOTEDSTRING:http_referrer}|-) (%{QUOTEDSTRING:user_agent}) \"(%{HOSTPORT:upstream_addr}|-)\" (%{NUMBER:upstream_st  
atus}|-) (%{NUMBER:upstream_response_time}|-) (%{{FORWORD:x_forword_for}|-)" }  
            remove_field => ["message"]  
            remove_field => ["agent","ecs","host"]  
            
IPV4 (?<![0-9])(?:(?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5])[.](?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5])[.](?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5])[.](?:[0-1]?[0-9]{1,2}|2[0-4][0-9]|25[0-5]))(?![0-9])  
HOSTNAME \b(?:[0-9A-Za-z][0-9A-Za-z-]{0,62})(?:\.(?:[0-9A-Za-z][0-9A-Za-z-]{0,62}))*(\.?|\b)  
POSINT \b(?:[1-9][0-9]*)\b  
USERNAME [a-zA-Z0-9._-]+  
HTTPDATE %{MONTHDAY}/%{MONTH}/%{YEAR}:%{TIME} %{INT}  
MONTHDAY (?:(?:0[1-9])|(?:[12][0-9])|(?:3[01])|[1-9])  
MONTH \b(?:[Jj]an(?:uary|uar)?|[Ff]eb(?:ruary|ruar)?|[Mm](?:a|ä)?r(?:ch|z)?|[Aa]pr(?:il)?|[Mm]a(?:y|i)?|[Jj]un(?:e|i)?|[Jj]ul(?:y)?|[Aa]ug(?:ust)?|[Ss]ep(?:tember)?|[Oo](?:c|k)?t(?:ober)?|[Nn]ov(?:ember)?|[Dd]e(?:c|z)(?:ember)?)\b  
YEAR (?>\d\d){1,2}  
TIME (?!<[0-9])%{HOUR}:%{MINUTE}(?::%{SECOND})(?![0-9])  
HOUR (?:2[0123]|[01]?[0-9])  
MINUTE (?:[0-5][0-9])  
SECOND (?:(?:[0-5]?[0-9]|60)(?:[:.,][0-9]+)?)  
INT (?:[+-]?(?:[0-9]+))  
WORD \b\w+\b  
NOTSPACE \S+  
NUMBER (?:%{BASE10NUM})  
BASE10NUM (?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\.[0-9]+)?)|(?:\.[0-9]+)))  
QUOTEDSTRING (?>(?<!\\)(?>"(?>\\.|[^\\"]+)+"|""|(?>'(?>\\.|[^\\']+)+')|''|(?>`(?>\\.|[^\\`]+)+`)|``))  
#custom  
FORWORD (?:%{IPV4}[,]?[ ]?)+|%{WORD}  
注意：  
语法认证网站http://grokdebug.herokuapp.com/的patten和我本地版本存在不匹配问题，导致172.19.2.50:8087无法用%{HOSTPORT}来匹配，只能用%{IPORHOST}:%{POSINT}来匹配  
这是网站的版本HOSTPORT (?:%{IPORHOST=~/\./}:%{POSINT})  
这是我软件的HOSTPORT %{IPORHOST}:%{POSINT}  
语法认证只能作为一个参考，或者从patterns连接比较，最终还要以安装的logstash版本为准  
```  



参考：  

https://www.cnblogs.com/Orgliny/p/5592186.html  







# elasticsearch  

## ES查询之刨根问底  

发布时间： 2019-03-03 15:16:35  

　　昨天有一个需求，就是想要根据某个网关url做过滤，获取其下面所有的上下文nginx日志；如果直接"query":"https://XXX/YYY/ZZZ"发现有问题，啥也查不出来，后来仁杰指出来需要使用“”括起来，果然这样就变成了前后匹配的模糊查询了。但是继续，我发现如果我指定了字段"query":"request:\"http://XXX/YYY/ZZZ\""就啥也返回不了了；但是如果是换位message字段则可以；后来发现原来是因为request的类型是keywords，message的类型是text，所以request必须是全匹配，message是模糊匹配。  

**第一部 分词**  

　　但是为什么需要添加“https://XXX/YY/ZZZ/”号呢？如果不添加会怎样，导致分词吗，如果分词正常应该可以查出来啊。后来发现之所以查不出来数据，是因为当时查询的querystirng最后一位带了"/"导致的，如果把最后一位的"/"给删掉就可以查出数据了；但是这个查询是经过ES的分词的，将会被解析为XXX，/YY/以及ZZZ，所以此时查询是分词查询；如果是想要不分词，进行完整匹配（ES不进行分词），需要添加双引号。如果获知分词的过程？就是通过index/_validate/query?explain，所谓的validate-query模式来进行查询解释。  
　　通过validate-query模式的查询，你会发现如果直接查（不指定字段），将会是所有的字段对所有的分词进行笛卡尔积的匹配；所以构建的查询语句也是十分复杂，这里你就会发现指定字段重要性了。  

**第二部 索引**  

　　什么是索引？引用一下我之前的一篇博文来说明：  

　　索引扮演的角色其实并不是存储，而是“索引”，看起来有点傻，但是其实我之前一直理解索引是存储，其实从命名上可以看出来，索引其实是分片的索引，分片的字典，记录了每个分片的位置，索引范围；当需要查询的时候，可以定位到对应的分片来进行数据操作；最后进行汇总。所以index本质作用就是记录分片；所谓查询，有向无环图（DAG）都是基于index来进行分析绘制的，然后基于该图下放数据操作。  

　　索引模板，就是指定了索引的属性，包括分片数，副本数等物理属性以及字段名称以及字段类型等信息。索引模板指定了索引模式，一般都是前缀模式，例如：app_log-*，这样每当创建了一个app_log-开头的索引（比如app_log-2019-3-3），就会按照该模板的属性进行创建。  

　　下面说一下索引的生命周期，这里包括热盘到冷盘，热盘到冷盘可以通过**box_type**来实现，创建的时候，指定分布部署的节点的类型（需要事先为每个集群节点设置节点类型），定时通过修改配置来对分片进行迁移到另外节点类型；  

　　生命周期的最后操作是：删除，关闭以及归档三种，删除就是DELETE指令，关闭就是_close，这个除了会占用硬盘空间之外不会有别的影响，好处就是可以随时打开参与查询；最后一个是归档，可以将索引归档到S3，hdfs等存储引擎。这些归档文件将会被ES系统记录；此时可以将索引删除（导致分片一起删除）；日后如果有需要可以通过_restore的api进行重新导入。  

**第三部 查询和解析**  

　　字段配置中有一个属性：index，默认是“analyzed”，代表存入文档后，将会被分词保存，同时统一转化为小写；另外一个值是not_analyzed，这样的文档里面保存的就是原始的文档内容。是的，所有的这一切都是5.0之前的配置，5.0之后，string+analyzed被text替换，string+not_analyzed被keyword替换，完美。  

　　term查询  

　　代表完全匹配，什么是完全匹配，是指索引保存的分词中的一个能够完全一致，和分词中的一个完全一样（不是模糊匹配）；**term查询不会对查询字符串进行分词**；这里牵涉到term查询中指定的字段类型是keyword还是text，如果是keyword，那么分词只有一个，就是文档全文本身，此时可以理解为有的字段，在查询字符串必须要和文档全部一致才可以；如果是text，那么将会根据内置规则进行分词，此时term查询的匹配就是要能够匹配分词后的一个即可；  

　　match查询  

　　**match查询则是会对查询字符串进行分词**，然后和索引的字段分词进行逐个匹配（又是一个笛卡尔积）。所以，你会发现其实ES查询本质面对的分词，文档其实只是一个PUT的概念，一旦存入了ES之后，就会被进行分解为分词（除非keyword，唯一的分词是文档本身）。下一个是match_phase，这个查询和match一样都是解析分词查询，但是增加了所有分词必须都匹配，而且分词的顺序要一致（可以指定距离slop）。  

　　query_string查询  

　　那么match和query_string的差别在哪里？**在于后者有谓词表达式**。其实如果是没有谓词表达式，那么query_string就是和match一样，如果没有谓词，查询字段用双引号包裹，其实就和match_phrase是一样的，query_string的强大就在于一个查询可以完成match和match_phrase的统一，同时还额外提供了谓词表达式做多条件匹配。  

**第四部 文档写入ES流程**  

　　逐个流程可以被进一步细化：PUT了一个文档之后，首先将会进行字段级别处理，一般是在logstash中进行，对于文档中的字段通过正则等规则进行提取，提取的目的是填充到各个字段；然后对于各个字段里面保存的内容，其实是所提取出来的子文档的分词，是的，字段保存的是分词，不再内容或者子内容，而是分词。文档的概念也不是原始的字符串，而是所有的字段以及字段分词的抽象；那么查询的时候其实都是基于对于查询字符串的分词（可能是不解析，比如使用“”，那就是一个分词）和字段分词（可能是keyword，那么就只有一个分词）进行匹配；查询字符串的解析有良心的使用者指定一下字段，没良心的不指定，那么就是所有的查询分词和所有的字段分词进行笛卡尔积形式的匹配，如果匹配的上，那么该文档就被返回。  

**第五部 总结和其他**  

　　索引里面存的分片信息；分片里面的存的是文档信息，文档里面存的字段信息，字段里面存的是分词信息。基本就是这么个层级关系；为什么要指定这么多层，就是为了能够细粒度定位数据，减少查询的面积。  
　　_all字段是一个虚拟字段，它将会打破字段这个层级，将所有的字段的分词都集成到一起，形成一个新的虚拟字段：_all。设置_all是一个字段级别的设置，在每个字段下面设置_all是否可用，因为会损耗性能，所以默认是关闭。但是_all在6.0之后将会被废弃，类似功能由copy_to参数来实现。  

# kibana  

## [使用kibana分析nginx日志](https://blog.csdn.net/weixin_43342753/article/details/89340022)  

## [kibana操作及nginx日志分析图表创建](https://blog.51cto.com/hnr520/1845900)  

## [Kibana 用户手册](https://www.elastic.co/guide/cn/kibana/current/index.html)  

## 可视化  

### pie饼图  

nginx返回码的饼图  

Split slices  切片  

Split chart  柱形图切片  

## 饼图  

饼图的切片大小由 *metrics* 聚合决定，下列聚合可用于饼图：  

- ***\*Count\****  

  [*count*](https://www.elastic.co/guide/en/elasticsearch/reference/6.0/search-aggregations-metrics-valuecount-aggregation.html) 聚合返回所选索引模式中元素的原始数量。  

- ***\*Sum\****  

  [*sum*](https://www.elastic.co/guide/en/elasticsearch/reference/6.0/search-aggregations-metrics-sum-aggregation.html) 聚合返回一个数值型字段的总和。从下拉框选择一个字段。  

- ***\*Unique Count\****  

  [*cardinality*](https://www.elastic.co/guide/en/elasticsearch/reference/6.0/search-aggregations-metrics-cardinality-aggregation.html) 聚合返回一个字段中唯一值的数量。从下拉列表选择一个字段。  

在 **Custom Label** 字段中输入一个字符串来修改显示标签。  

*桶* 聚合用于决定从数据集抽取何种信息。  

在选择一个桶聚合之前，需要知道是否为单个图或组合图的X轴或Y轴定义桶。一个组合图必须在所有其他聚合之前执行。当划分一个图时，可以通过点击 **Rows | Columns** 选择器，来改变划分是显示为一行还是一列。  

可以为饼图指定下列任意桶聚合：  

- ***\*Date Histogram\****  

  一个 [*date histogram*](https://www.elastic.co/guide/en/elasticsearch/reference/6.0/search-aggregations-bucket-datehistogram-aggregation.html) 从一个数值型字段构建，并按日期组织。可以为间隔指定一个按秒、分钟、小时、天、周、月或年的时间段。也可以指定一个自定义的时间区间，只需选择 **Custom** 作为间隔，并在文本字段中指定一个数字和一个时间单位即可。对于自定义间隔时间单位，**s** 表示秒， **m** 表示分钟，**h** 表示小时， **d** 表示天， **w** 表示周， **y** 表示年。不同单位支持不同的精度级别，最低为一秒。  

- ***\*Histogram\****  

  一个标准的 [*histogram*](https://www.elastic.co/guide/en/elasticsearch/reference/6.0/search-aggregations-bucket-histogram-aggregation.html) 从一个数值型字段构建，并为该字段指定一个整数类型的间隔，选择 **Show empty buckets** 复选框可在直方图中包括空的间隔。  

- ***\*Range\****  

  通过一个 [*range*](https://www.elastic.co/guide/en/elasticsearch/reference/6.0/search-aggregations-bucket-range-aggregation.html) 聚合，可以为一个数值型字段指定值的范围。点击 **Add Range** 增加一个范围聚合，点击红色的 **(x)** 符号来删除一个范围。  

- ***\*Date Range\****  

  [*date range*](https://www.elastic.co/guide/en/elasticsearch/reference/6.0/search-aggregations-bucket-daterange-aggregation.html) 聚合展示在指定日期范围内的值。可通过 [*date math*](https://www.elastic.co/guide/en/elasticsearch/reference/6.0/common-options.html#date-math) 表达式来指定日期范围。点击 **Add Range** 增加一个范围聚合，点击红色的 **(/)** 符号来删除一个范围。  

- ***\*IPv4 Range\****  

  [*IPv4 range*](https://www.elastic.co/guide/en/elasticsearch/reference/6.0/search-aggregations-bucket-iprange-aggregation.html) 聚合支持指定IPV4地址范围。点击 **Add Range** 增加一组范围端点，点击红色的 **(/)** 符号移除范围。  

- ***\*Terms\****  

  [*terms*](https://www.elastic.co/guide/en/elasticsearch/reference/6.0/search-aggregations-bucket-terms-aggregation.html) 聚合支持指定要显示的给定字段的头部或尾部 *n* 个元素，并按数量或自定义指标进行排序。  

- ***\*Filters\****  

  可以为数据指定一系列 [*filters*](https://www.elastic.co/guide/en/elasticsearch/reference/6.0/search-aggregations-bucket-filters-aggregation.html) 。支持通过一个查询串或者 JSON 格式来指定一个过滤器，就像在 Discover 搜索框中一样。点击 **Add Filter** 来增加另一个过滤器。点击 ![labelbutton](https://www.elastic.co/guide/cn/kibana/current/images/labelbutton.png) **label** 按钮打开标签字段，输入一个可显示在视图中的名称。  

- ***\*Significant Terms\****  

  显示试验 [*significant terms*](https://www.elastic.co/guide/en/elasticsearch/reference/6.0/search-aggregations-bucket-significantterms-aggregation.html) 聚合的结果。**Size** 参数的值定义了该聚合返回的实体数量。  

一旦指定了一个 bucket 类型的聚合，就可以定义子 bucket 来优化视图。点击 **+ Add sub-buckets** 来定义一个子 bucket，然后选择 **Split Rows** 或 **Split Table** ，再从类型列表中选择一种聚合。  

当在坐标轴上定义好多个聚合以后，就可以使用向上或向下键翻到合适的聚合类型，以更改聚合优先级。  

点击每个标签旁边的色点来显示 *颜色选择器* ，可以自定义视图的颜色。  

![An array of color dots that users can select](https://www.elastic.co/guide/cn/kibana/current/images/color-picker.png)  

在 **Custom Label** 字段输入一个字符串可修改显示标签。  

可以点击 **Advanced** 链接显示指标或桶聚合的更多自定义选项：  

- ***\*Exclude Pattern\****  

  从结果中排除该字段指定的模式。  

- ***\*Include Pattern\****  

  在结果中包括该字段所指定的模式。  

- ***\*JSON Input\****  

  一个文本字段，可以通过加入指定的 JSON 格式属性与聚合定义合并，示例如下：  

```shell  
{ "script" : "doc['grade'].value * 1.2" }  
```  



在 Elasticsearch 1.4.3及以后版本中，该功能需要打开 [dynamic Groovy scripting](https://www.elastic.co/guide/en/elasticsearch/reference/6.0/modules-scripting.html) 。  

这些选项是否可用取决于所选的聚合。  

选择 **Options** 标签来改变表格的下列方面：  

- ***\*Donut\****  

  显示为切片环状图，而不是切片饼状图。  

- ***\*Show Tooltip\****  

  勾选此项开启显示提示语。  

在修改选项后，点击 **Apply changes** 按钮更新视图，或者点击 **Discard changes** 按钮保持视图为当前状态。  


  













## 地图(7.6版本)  



参考:https://www.elastic.co/guide/en/kibana/7.6/maps.html  

两个入口：  

[Visualize](https://www.elastic.co/guide/en/kibana/7.6/visualize.html) » Heat map和MAPS  

分别为热点图和地图全局  

## Heat map  

Display graphical representations of data where the individual values are represented by colors. Use heat maps when your data set includes categorical data. For example, use a heat map to see the flights of origin countries compared to destination countries using the sample flight data.  

显示数据的图形表示，其中各个值由颜色表示。当您的数据集包括分类数据时，请使用热图。例如，使用热图查看原始国家的航班与使用示例航班数据的目的地国家的航班。  

#### Build a heat map  

To display your data on the heat map, use the supported aggregations.  

Heat maps support the following aggregations:  

要在热点图上显示数据，请使用支持的聚合。  

热点图支持以下聚合:  

- [Metric](https://www.elastic.co/guide/en/kibana/7.6/supported-aggregations.html#visualize-metric-aggregations)  

- [Parent pipeline](https://www.elastic.co/guide/en/kibana/7.6/supported-aggregations.html#visualize-parent-pipeline-aggregations)  

- [Sibling pipeline](https://www.elastic.co/guide/en/kibana/7.6/supported-aggregations.html#visualize-sibling-pipeline-aggregations)  

- [Bucket](https://www.elastic.co/guide/en/kibana/7.6/supported-aggregations.html#visualize-bucket-aggregations)  

  聚合参数https://www.elastic.co/guide/cn/kibana/current/heatmap-chart.html  

#### Change the color ranges  

When only one color displays on the heat map, you might need to change the color ranges.  

To specify the number of color ranges:  

1. Click **Options**.  
2. Enter the **Number of colors** to display.  

To specify custom ranges:  

1. Click **Options**.  
2. Select **Use custom ranges**.  
3. Enter the ranges to display.  




